{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63b975e282448949714d39ed97c65ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "os.chdir(\"/work/pi_jensen_umass_edu/jnainani_umass_edu/CircuitAnalysisSAEs\")\n",
    "import json\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from circ4latents import data_gen\n",
    "from functools import partial\n",
    "import einops\n",
    "\n",
    "# Function to manage CUDA memory and clean up\n",
    "def cleanup_cuda():\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "# cleanup_cuda()\n",
    "# Load the config\n",
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-9b\", device=device, cache_dir=hf_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae21 = SAE.from_pretrained(release=\"gemma-scope-9b-pt-res\", sae_id=f\"layer_21/width_16k/average_l0_129\", device=device)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple String:Int Dictionary Key Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_diff: 3.566526412963867\n",
      "corr_diff: -4.803060531616211\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Expanding the name pool with a larger set of names\n",
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\"\n",
    "]\n",
    "\n",
    "for name in extended_name_pool:\n",
    "    assert len(model.tokenizer.encode(name)) == 2, f\"Name {name} has more than 1 token\"\n",
    "\n",
    "# Function to generate the dataset with correct and incorrect keying into dictionaries\n",
    "def generate_extended_dataset(name_pool, num_samples=5):\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select 5 names from the pool\n",
    "        selected_names = random.sample(name_pool, 5)\n",
    "        # Assign random ages to the selected names\n",
    "        age_dict = {name: random.randint(10, 19) for name in selected_names}\n",
    "        \n",
    "        # Create a correct example\n",
    "        correct_name = random.choice(list(age_dict.keys()))\n",
    "        correct_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> var = age[\"{correct_name}\"]\\n'\n",
    "        correct_response = age_dict[correct_name]\n",
    "        correct_token = str(correct_response)[0]\n",
    "        \n",
    "        # Create an incorrect example with a name not in the dictionary\n",
    "        incorrect_name = random.choice([name for name in name_pool if name not in age_dict])\n",
    "        incorrect_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> var = age[\"{incorrect_name}\"]\\n'\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "        \n",
    "        # Append the pair of correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# Generate the extended dataset\n",
    "json_dataset = generate_extended_dataset(extended_name_pool, num_samples=100)\n",
    "\n",
    "# Output the JSON structure\n",
    "\n",
    "# %%\n",
    "clean_prompts = []\n",
    "corr_prompts = []\n",
    "\n",
    "answer_token = model.to_single_token(\">>>\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in json_dataset[:50]:\n",
    "    corr_prompts.append(item[\"correct\"][\"prompt\"])\n",
    "    clean_prompts.append(item[\"error\"][\"prompt\"])\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompts)\n",
    "corr_tokens = model.to_tokens(corr_prompts)\n",
    "\n",
    "# %%\n",
    "def logit_diff_fn(logits):\n",
    "    err = logits[:, -1, traceback_token]\n",
    "    no_err = logits[:, -1, answer_token]\n",
    "    return (err - no_err).mean()\n",
    "\n",
    "# Disable gradients for all parameters\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "# # Compute logits for clean and corrupted samples\n",
    "logits = model(clean_tokens)\n",
    "clean_diff = logit_diff_fn(logits)\n",
    "\n",
    "logits = model(corr_tokens)\n",
    "corr_diff = logit_diff_fn(logits)\n",
    "\n",
    "print(f\"clean_diff: {clean_diff}\")\n",
    "print(f\"corr_diff: {corr_diff}\")\n",
    "\n",
    "# # Cleanup\n",
    "del logits\n",
    "cleanup_cuda()\n",
    "\n",
    "# # Define error type metric\n",
    "def _err_type_metric(logits, clean_logit_diff, corr_logit_diff):\n",
    "    patched_logit_diff = logit_diff_fn(logits)\n",
    "    return (patched_logit_diff - corr_logit_diff) / (clean_logit_diff - corr_logit_diff)\n",
    "\n",
    "err_metric_denoising = partial(_err_type_metric, clean_logit_diff=clean_diff, corr_logit_diff=corr_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> age = {'Benjamin': 13, 'Chris': 16, 'Lilly': 12, 'Charlie': 10, 'Emma': 16}\n",
      ">>> var = age[\"Isabella\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(json_dataset[0]['error']['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attr calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import ActivationCache, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "# from torchtyping import TensorType as TT\n",
    "\n",
    "def get_cache_fwd_and_bwd(\n",
    "    model,\n",
    "    tokens,\n",
    "    metric,\n",
    "    sae,\n",
    "    error_term: bool = True,\n",
    "    retain_graph: bool = True\n",
    "):\n",
    "    # torch.set_grad_enabled(True)\n",
    "    model.reset_hooks()\n",
    "    # model.reset_saes()\n",
    "    cache = {}\n",
    "    grad_cache = {}\n",
    "    filter_base_acts = lambda name: \"blocks.21.hook_resid_post\" in name\n",
    "    # filter_sae_acts = lambda name: \"hook_sae_acts_post\" in name\n",
    "\n",
    "    def forward_cache_hook(act, hook):\n",
    "        act.requires_grad_(True)\n",
    "        # act.retain_graph()\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    def backward_cache_hook(grad, hook):\n",
    "        grad.requires_grad_(True)\n",
    "        # grad.retain_graph()\n",
    "        grad_cache[hook.name] = grad.detach()\n",
    "\n",
    "    # sae.use_error_term = error_term\n",
    "    # model.add_sae(sae)\n",
    "    model.add_hook(filter_base_acts, forward_cache_hook, \"fwd\")\n",
    "    model.add_hook(filter_base_acts, backward_cache_hook, \"bwd\")\n",
    "    # logits = run_with_saes_filtered(tokens, [model.tokenizer.bos_token_id, model.tokenizer.eos_token_id, model.tokenizer.pad_token_id], model, [sae])\n",
    "    value = metric(model(tokens)) #logits)\n",
    "    value.backward() #retain_graph=retain_graph)\n",
    "\n",
    "    model.reset_hooks()\n",
    "    # model.reset_saes()\n",
    "    # torch.set_grad_enabled(False)\n",
    "    return (\n",
    "        value,\n",
    "        ActivationCache(cache, model),\n",
    "        ActivationCache(grad_cache, model),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_value, clean_cache, _ = get_cache_fwd_and_bwd(model, clean_tokens, err_metric_denoising, sae21)\n",
    "print(\"Clean Value:\", clean_value)\n",
    "print(\"Clean Activations Cached:\", len(clean_cache))\n",
    "# print(\"Clean Gradients Cached:\", len(clean_grad_cache))\n",
    "\n",
    "corrupted_value, corrupted_cache, corrupted_grad_cache = get_cache_fwd_and_bwd(model, corr_tokens, err_metric_denoising, sae21)\n",
    "print(\"Corrupted Value:\", corrupted_value)\n",
    "print(\"Corrupted Activations Cached:\", len(corrupted_cache))\n",
    "print(\"Corrupted Gradients Cached:\", len(corrupted_grad_cache))\n",
    "\n",
    "# # Cleanup\n",
    "del clean_value, corrupted_value\n",
    "cleanup_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.8120e-07, -3.2566e-07, -1.0438e-06,  ...,  7.1768e-07,\n",
       "          -9.9885e-07,  8.6916e-07],\n",
       "         [-3.1879e-07, -6.1671e-09,  1.5720e-07,  ...,  1.1308e-07,\n",
       "           1.0026e-07,  5.5729e-08],\n",
       "         [-7.8373e-08, -9.4730e-08, -2.5211e-07,  ...,  1.4263e-07,\n",
       "          -7.1474e-07,  1.2810e-07],\n",
       "         ...,\n",
       "         [ 1.6529e-06,  1.1248e-06, -4.2053e-07,  ..., -2.3218e-06,\n",
       "           3.5809e-07, -4.9164e-07],\n",
       "         [ 5.5071e-06,  9.5625e-08, -2.0255e-06,  ..., -3.0415e-06,\n",
       "           6.6743e-06, -1.9352e-06],\n",
       "         [-1.2636e-05, -2.7388e-05,  3.0116e-06,  ...,  1.2309e-05,\n",
       "           2.0280e-05, -8.1272e-06]],\n",
       "\n",
       "        [[-4.9830e-07, -1.5768e-07, -1.0518e-06,  ...,  5.8069e-07,\n",
       "          -9.4853e-07,  1.0167e-06],\n",
       "         [-3.2971e-07, -1.6169e-08,  1.6466e-07,  ...,  1.2835e-07,\n",
       "           1.0842e-07,  5.0115e-08],\n",
       "         [-1.0604e-07, -1.4316e-07, -2.7018e-07,  ...,  8.6350e-08,\n",
       "          -7.1302e-07,  7.1841e-08],\n",
       "         ...,\n",
       "         [ 6.9058e-07,  8.4241e-07, -1.0532e-06,  ..., -1.6721e-06,\n",
       "           6.1292e-07, -1.2897e-06],\n",
       "         [ 4.3273e-06, -1.0278e-07, -3.2053e-06,  ..., -2.4711e-06,\n",
       "           6.7633e-06, -3.1260e-06],\n",
       "         [-1.3252e-05, -2.7303e-05,  4.7359e-06,  ...,  1.1787e-05,\n",
       "           1.7936e-05, -7.2960e-06]],\n",
       "\n",
       "        [[-9.7138e-08, -4.9149e-07, -7.6616e-07,  ...,  9.5775e-07,\n",
       "          -9.3704e-07,  3.9404e-07],\n",
       "         [-3.4816e-07,  1.3596e-08,  1.1366e-07,  ...,  1.0653e-07,\n",
       "           3.6722e-08,  1.3388e-08],\n",
       "         [-7.7473e-08, -5.7383e-08, -2.9815e-07,  ...,  1.0720e-07,\n",
       "          -8.0323e-07,  5.7256e-08],\n",
       "         ...,\n",
       "         [ 1.7167e-06,  7.4786e-07, -9.0556e-07,  ..., -1.0505e-06,\n",
       "          -1.4845e-06,  1.0069e-06],\n",
       "         [ 3.3925e-06,  7.6260e-07, -1.9029e-07,  ..., -2.5217e-06,\n",
       "           5.1077e-06,  5.5587e-07],\n",
       "         [-1.3180e-05, -2.6226e-05, -3.1364e-08,  ...,  1.2161e-05,\n",
       "           1.7017e-05, -1.4970e-06]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.4691e-07, -3.6315e-07, -7.0815e-07,  ...,  5.9781e-07,\n",
       "          -1.0729e-06,  6.0415e-07],\n",
       "         [-3.6268e-07,  5.1302e-08,  1.7516e-07,  ...,  1.6067e-07,\n",
       "           8.0506e-08,  3.8892e-08],\n",
       "         [-7.6956e-08, -6.3057e-08, -2.4494e-07,  ...,  7.9281e-08,\n",
       "          -7.3464e-07,  4.0038e-08],\n",
       "         ...,\n",
       "         [ 2.1892e-06,  1.0245e-06, -5.2278e-08,  ..., -3.1833e-06,\n",
       "           6.7692e-07, -5.9407e-07],\n",
       "         [ 4.6625e-06, -6.4602e-07, -1.3270e-06,  ..., -3.4453e-06,\n",
       "           5.6753e-06, -3.1358e-07],\n",
       "         [-1.1376e-05, -2.5327e-05,  5.2166e-06,  ...,  9.8105e-06,\n",
       "           1.5865e-05, -1.9572e-06]],\n",
       "\n",
       "        [[-4.6718e-07, -3.0109e-07, -8.1524e-07,  ...,  4.9865e-07,\n",
       "          -8.8969e-07,  6.1116e-07],\n",
       "         [-3.5619e-07,  3.0094e-09,  1.2026e-07,  ...,  1.2627e-07,\n",
       "           1.1125e-07,  7.0419e-08],\n",
       "         [-8.7798e-08, -1.3262e-07, -3.0717e-07,  ...,  1.0715e-07,\n",
       "          -7.1739e-07,  9.0596e-08],\n",
       "         ...,\n",
       "         [ 1.5082e-06,  1.0885e-06, -1.2133e-06,  ..., -1.0460e-06,\n",
       "           6.0857e-07, -1.5830e-06],\n",
       "         [ 4.4196e-06,  2.9839e-08, -2.2507e-06,  ..., -3.3211e-06,\n",
       "           5.9295e-06, -4.9264e-07],\n",
       "         [-1.0719e-05, -2.6138e-05,  3.1945e-06,  ...,  8.9909e-06,\n",
       "           1.7250e-05, -7.5218e-06]],\n",
       "\n",
       "        [[-3.2346e-07, -4.2009e-07, -8.7366e-07,  ...,  6.0076e-07,\n",
       "          -1.1373e-06,  8.1521e-07],\n",
       "         [-3.5587e-07, -3.5227e-08,  1.6823e-07,  ...,  1.0649e-07,\n",
       "           1.0523e-07,  3.2635e-09],\n",
       "         [-6.3452e-08, -8.1690e-08, -3.4345e-07,  ...,  1.3213e-07,\n",
       "          -7.2678e-07,  8.0499e-08],\n",
       "         ...,\n",
       "         [ 1.2545e-06,  5.6609e-07, -8.4609e-07,  ..., -1.6347e-06,\n",
       "           2.2716e-07, -5.4248e-07],\n",
       "         [ 5.1082e-06, -2.8708e-07, -2.6609e-06,  ..., -4.1101e-06,\n",
       "           6.3837e-06, -1.8627e-06],\n",
       "         [-1.2698e-05, -2.6981e-05,  3.4524e-06,  ...,  9.8402e-06,\n",
       "           1.9746e-05, -7.7184e-06]]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_grad_cache['blocks.21.hook_resid_post'][:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 66, 16384]) torch.Size([50, 66, 16384])\n",
      "torch.Size([50, 66, 16384])\n"
     ]
    }
   ],
   "source": [
    "sae_acts = sae21.encode(clean_cache['blocks.21.hook_resid_post'][:, 1:, :])\n",
    "sae_acts_corr = sae21.encode(corrupted_cache['blocks.21.hook_resid_post'][:, 1:, :])\n",
    "print(sae_acts.shape, sae_acts_corr.shape)\n",
    "\n",
    "sae_grad_cache = torch.einsum('bij,kj->bik', corrupted_grad_cache['blocks.21.hook_resid_post'][:, 1:, :], sae21.W_dec)\n",
    "print(sae_grad_cache.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3237,  3271,  8156,  8233, 11219, 10711,  9585,  8535,  5795, 13756,\n",
      "         3925,   430, 10160,  6290, 11774,  9654,  3377, 13275, 13163,   111,\n",
      "         8311, 15795, 13862, 10350,  7892, 13218, 10618, 14598, 12223,  4766,\n",
      "         5577,  3395, 11725, 14495, 15571,  7482,  9768,  9960, 13842,  2782,\n",
      "         5804,  6840, 11779, 15430,  5498, 11712, 14638, 13839, 10587,  7675,\n",
      "         3807,  4804,  2405, 14331,  6855,  8696,   691, 10110,  1662, 12189,\n",
      "         4337,  1407,  8180, 12151,  7950, 14786,  9190, 12440,  9858, 10916,\n",
      "           37, 13831,  2187,  9304, 14499,  3818, 10219,  2120,   940, 12739,\n",
      "        15805,  5615,  7988,  7237,  6098,  2239,  6606,  2314,  4022, 14229,\n",
      "         6646, 10719,  7381,  8680,  1436, 10023, 14613,  3863, 15777, 12609],\n",
      "       device='cuda:0') tensor([ 0.0732,  0.0403,  0.0253,  0.0250,  0.0206,  0.0195,  0.0191,  0.0190,\n",
      "         0.0115,  0.0100,  0.0098,  0.0088,  0.0087,  0.0085,  0.0082,  0.0071,\n",
      "         0.0064,  0.0063,  0.0062, -0.0058,  0.0055, -0.0055,  0.0054,  0.0054,\n",
      "         0.0054,  0.0054,  0.0053,  0.0050,  0.0050,  0.0049,  0.0048,  0.0047,\n",
      "         0.0042, -0.0040,  0.0040,  0.0039, -0.0038, -0.0034,  0.0031,  0.0029,\n",
      "         0.0028, -0.0027,  0.0027,  0.0026,  0.0025,  0.0024, -0.0024,  0.0024,\n",
      "        -0.0023,  0.0022,  0.0021, -0.0020, -0.0020, -0.0020,  0.0020,  0.0020,\n",
      "         0.0019, -0.0019,  0.0018,  0.0018,  0.0017, -0.0017, -0.0017, -0.0015,\n",
      "         0.0015,  0.0015,  0.0015, -0.0015,  0.0015,  0.0015,  0.0014, -0.0014,\n",
      "        -0.0013, -0.0013,  0.0013,  0.0012,  0.0012, -0.0012, -0.0012,  0.0012,\n",
      "         0.0012,  0.0011,  0.0011,  0.0011,  0.0011,  0.0011,  0.0010,  0.0010,\n",
      "         0.0010, -0.0010, -0.0009,  0.0009,  0.0009,  0.0009,  0.0009,  0.0009,\n",
      "         0.0008,  0.0008,  0.0008,  0.0008], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "K = 100\n",
    "# Residual attribution calculation only for the selected positions\n",
    "residual_attr_final = einops.reduce(\n",
    "    sae_grad_cache * (sae_acts - sae_acts_corr),\n",
    "    \"batch pos n_features -> n_features\",\n",
    "    \"sum\",\n",
    ")\n",
    "# Get the top K features based on the absolute values\n",
    "abs_residual_attr_final = torch.abs(residual_attr_final)\n",
    "top_feats = torch.topk(abs_residual_attr_final, K)\n",
    "top_indices = top_feats.indices\n",
    "top_values = residual_attr_final[top_indices] \n",
    "print(top_indices, top_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4175, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(top_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_saes_filtered_cache(tokens, filtered_ids, model, saes):\n",
    "    # Ensure tokens are a torch.Tensor\n",
    "    if not isinstance(tokens, torch.Tensor):\n",
    "        tokens = torch.tensor(tokens).to(model.cfg.device)  # Move to the device of the model\n",
    "\n",
    "    # Create a mask where True indicates positions to modify\n",
    "    mask = torch.ones_like(tokens, dtype=torch.bool)\n",
    "    for token_id in filtered_ids:\n",
    "        mask &= tokens != token_id\n",
    "\n",
    "\n",
    "    # Expand the mask once, so it matches the shape [batch_size, seq_len, 1]\n",
    "    mask_expanded = mask.unsqueeze(-1)  # Expand to allow broadcasting\n",
    "    mask_expanded = mask_expanded.to(model.cfg.device)  # Move the mask to the same device as the model\n",
    "\n",
    "    # Dictionary to store the modified activations\n",
    "    sae_outs = {}\n",
    "\n",
    "\n",
    "    # For each SAE, add the appropriate hook\n",
    "    for sae in saes:\n",
    "        hook_point = sae.cfg.hook_name\n",
    "\n",
    "\n",
    "        # Define the filtered hook function (optimized)\n",
    "        def filtered_hook(act, hook, sae=sae, mask_expanded=mask_expanded):\n",
    "            # Apply the SAE only where mask_expanded is True\n",
    "            enc_sae = sae.encode(act)  # Call the SAE once\n",
    "            # Store the updated activation in the dictionary\n",
    "            sae_outs[hook.name] = enc_sae.detach().cpu() \n",
    "            modified_act = sae.decode(enc_sae)  # Call the SAE once\n",
    "            # In-place update where the mask is True\n",
    "            updated_act = torch.where(mask_expanded, modified_act, act)\n",
    "        \n",
    "            return updated_act\n",
    "\n",
    "\n",
    "        # Add the hook to the model\n",
    "        model.add_hook(hook_point, filtered_hook, dir='fwd')\n",
    "\n",
    "\n",
    "    # Run the model with the tokens (no gradients needed)\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens)\n",
    "\n",
    "\n",
    "    # Reset the hooks after computation to free memory\n",
    "    model.reset_hooks()\n",
    "\n",
    "\n",
    "    return logits, sae_outs  # Return logits and the updated activations\n",
    "\n",
    "\n",
    "def run_with_saes_latent_op_patch(new_tokens, filtered_ids, model, saes, cache, dict_feats):\n",
    "   # Ensure tokens are a torch.Tensor\n",
    "   if not isinstance(new_tokens, torch.Tensor):\n",
    "       new_tokens = torch.tensor(new_tokens).to(model.cfg.device)  # Move to the device of the model\n",
    "\n",
    "   # Create a mask where True indicates positions to modify\n",
    "   mask = torch.ones_like(new_tokens, dtype=torch.bool)\n",
    "   for token_id in filtered_ids:\n",
    "       mask &= new_tokens != token_id\n",
    "\n",
    "   # Expand the mask once, so it matches the shape [batch_size, seq_len, 1]\n",
    "   mask_expanded = mask.unsqueeze(-1)  # Expand to allow broadcasting\n",
    "   mask_expanded = mask_expanded.to(model.cfg.device)  # Move the mask to the same device as the model\n",
    "   # For each SAE, add the appropriate hook\n",
    "   for sae in saes:\n",
    "       hook_point = sae.cfg.hook_name\n",
    "\n",
    "       # Define the filtered hook function (optimized)\n",
    "       def filtered_hook(act, hook, sae=sae, mask_expanded=mask_expanded):\n",
    "           # Apply the SAE only where mask_expanded is True\n",
    "           enc_sae = sae.encode(act)  # Call the SAE once\n",
    "          \n",
    "           if hook.name in cache and hook.name in dict_feats:\n",
    "               prev_sae = cache[hook.name]  # Get cached activations from the cache\n",
    "               feature_indices = dict_feats[hook.name]  # Get the feature indices to patch\n",
    "\n",
    "               for feature_idx in range(sae.cfg.d_sae):\n",
    "                   if feature_idx in feature_indices:\n",
    "                       enc_sae[:, :, feature_idx] = prev_sae[:, :, feature_idx]\n",
    "\n",
    "           # After patching, decode the modified enc_sae\n",
    "           modified_act = sae.decode(enc_sae)\n",
    "\n",
    "           # In-place update where the mask is True\n",
    "           updated_act = torch.where(mask_expanded, modified_act, act)\n",
    "\n",
    "           return updated_act\n",
    "\n",
    "       # Add the hook to the model\n",
    "       model.add_hook(hook_point, filtered_hook, dir='fwd')\n",
    "\n",
    "   # Run the model with the tokens (no gradients needed)\n",
    "   with torch.no_grad():\n",
    "       logits = model(new_tokens)\n",
    "\n",
    "   # Reset the hooks after computation to free memory\n",
    "   model.reset_hooks()\n",
    "\n",
    "   return logits  # Return only the logits\n",
    "\n",
    "\n",
    "def run_with_saes_latent_op_patch_mean(new_tokens, filtered_ids, model, saes, mean_cache, dict_feats):\n",
    "   # Ensure tokens are a torch.Tensor\n",
    "   if not isinstance(new_tokens, torch.Tensor):\n",
    "       new_tokens = torch.tensor(new_tokens).to(model.cfg.device)  # Move to the device of the model\n",
    "\n",
    "   # Create a mask where True indicates positions to modify\n",
    "   mask = torch.ones_like(new_tokens, dtype=torch.bool)\n",
    "   for token_id in filtered_ids:\n",
    "       mask &= new_tokens != token_id\n",
    "\n",
    "   # Expand the mask once, so it matches the shape [batch_size, seq_len, 1]\n",
    "   mask_expanded = mask.unsqueeze(-1)  # Expand to allow broadcasting\n",
    "   mask_expanded = mask_expanded.to(model.cfg.device)  # Move the mask to the same device as the model\n",
    "   # For each SAE, add the appropriate hook\n",
    "   for sae in saes:\n",
    "       hook_point = sae.cfg.hook_name\n",
    "\n",
    "       # Define the filtered hook function (optimized)\n",
    "       def filtered_hook(act, hook, sae=sae, mask_expanded=mask_expanded):\n",
    "           # Apply the SAE only where mask_expanded is True\n",
    "           enc_sae = sae.encode(act)  # Call the SAE once\n",
    "          \n",
    "           if hook.name in mean_cache and hook.name in dict_feats:\n",
    "               prev_sae = mean_cache[hook.name]  # Get cached activations from the cache\n",
    "               feature_indices = dict_feats[hook.name]  # Get the feature indices to patch\n",
    "\n",
    "               for feature_idx in range(sae.cfg.d_sae):\n",
    "                   if feature_idx not in feature_indices:\n",
    "                       enc_sae[:, :, feature_idx] = prev_sae[:, feature_idx]\n",
    "\n",
    "           # After patching, decode the modified enc_sae\n",
    "           modified_act = sae.decode(enc_sae)\n",
    "\n",
    "           # In-place update where the mask is True\n",
    "           updated_act = torch.where(mask_expanded, modified_act, act)\n",
    "\n",
    "           return updated_act\n",
    "\n",
    "       # Add the hook to the model\n",
    "       model.add_hook(hook_point, filtered_hook, dir='fwd')\n",
    "\n",
    "   # Run the model with the tokens (no gradients needed)\n",
    "   with torch.no_grad():\n",
    "       logits = model(new_tokens)\n",
    "\n",
    "   # Reset the hooks after computation to free memory\n",
    "   model.reset_hooks()\n",
    "\n",
    "   return logits  # Return only the logits\n",
    "\n",
    "\n",
    "def run_with_saes_latent_op_patch_cache(new_tokens, filtered_ids, model, saes, cache, dict_feats):\n",
    "   # Ensure tokens are a torch.Tensor\n",
    "   if not isinstance(new_tokens, torch.Tensor):\n",
    "      new_tokens = torch.tensor(new_tokens).to(model.cfg.device)  # Move to the device of the model\n",
    "\n",
    "   # Create a mask where True indicates positions to modify\n",
    "   mask = torch.ones_like(new_tokens, dtype=torch.bool)\n",
    "   for token_id in filtered_ids:\n",
    "      mask &= new_tokens != token_id\n",
    "\n",
    "   # Expand the mask once, so it matches the shape [batch_size, seq_len, 1]\n",
    "   mask_expanded = mask.unsqueeze(-1)  # Expand to allow broadcasting\n",
    "   mask_expanded = mask_expanded.to(model.cfg.device)  # Move the mask to the same device as the model\n",
    "   sae_outs = {}\n",
    "   # For each SAE, add the appropriate hook\n",
    "   for sae in saes:\n",
    "      hook_point = sae.cfg.hook_name\n",
    "\n",
    "      # Define the filtered hook function (optimized)\n",
    "      def filtered_hook(act, hook, sae=sae, mask_expanded=mask_expanded):\n",
    "         # Apply the SAE only where mask_expanded is True\n",
    "         enc_sae = sae.encode(act)  # Call the SAE once\n",
    "         \n",
    "         if hook.name in cache and hook.name in dict_feats:\n",
    "            prev_sae = cache[hook.name]  # Get cached activations from the cache\n",
    "            feature_indices = dict_feats[hook.name]  # Get the feature indices to patch\n",
    "\n",
    "            for feature_idx in range(sae.cfg.d_sae):\n",
    "               if feature_idx in feature_indices:\n",
    "                  enc_sae[:, :, feature_idx] = prev_sae[:, :, feature_idx]\n",
    "         sae_outs[hook.name] = enc_sae.detach().cpu()\n",
    "         # After patching, decode the modified enc_sae\n",
    "         modified_act = sae.decode(enc_sae)\n",
    "\n",
    "         # In-place update where the mask is True\n",
    "         updated_act = torch.where(mask_expanded, modified_act, act)\n",
    "\n",
    "         return updated_act\n",
    "\n",
    "      # Add the hook to the model\n",
    "      model.add_hook(hook_point, filtered_hook, dir='fwd')\n",
    "\n",
    "   # Run the model with the tokens (no gradients needed)\n",
    "   with torch.no_grad():\n",
    "      logits = model(new_tokens)\n",
    "\n",
    "   # Reset the hooks after computation to free memory\n",
    "   model.reset_hooks()\n",
    "\n",
    "   return logits, sae_outs  # Return only the logits\n",
    "# def run_with_saes_latent_edge_patch(new_tokens, filtered_ids, model, saes, cache, sender_feats, receiver_feats):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_sae_diff: 0.44496282935142517\n"
     ]
    }
   ],
   "source": [
    "# %% clean cache \n",
    "filtered_ids = [model.tokenizer.bos_token_id]\n",
    "logits, clean_sae_cache = run_with_saes_filtered_cache(clean_tokens, filtered_ids=filtered_ids, model=model, saes=[sae21])\n",
    "clean_sae_diff = logit_diff_fn(logits)\n",
    "print(f\"clean_sae_diff: {clean_sae_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_sae_diff: -4.756180763244629\n"
     ]
    }
   ],
   "source": [
    "# %% corr cache \n",
    "logits, corr_sae_cache = run_with_saes_filtered_cache(corr_tokens, filtered_ids=filtered_ids, model=model, saes=[sae21])\n",
    "corr_sae_diff = logit_diff_fn(logits)\n",
    "print(f\"corr_sae_diff: {corr_sae_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.7968)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(corr_sae_cache['blocks.21.hook_resid_post'][0, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 67, 16384])\n",
      "torch.Size([67, 16384])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corr_sae_cache_means = {layer: sae_cache.mean(dim=0) for layer, sae_cache in corr_sae_cache.items()}\n",
    "\n",
    "print(corr_sae_cache['blocks.21.hook_resid_post'].shape)\n",
    "print(corr_sae_cache_means['blocks.21.hook_resid_post'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_feats = {\"blocks.21.hook_resid_post\":[ 3237,  3271,  8156,  8233, 11219, 10711,  9585,  8535,  5795, 13756,\n",
    "         3925,   430, 10160,  6290, 11774,  9654,  3377, 13275, 13163,   111,\n",
    "         8311, 15795, 13862, 10350,  7892, 13218, 10618, 14598, 12223,  4766,\n",
    "         5577,  3395, 11725, 14495, 15571,  7482,  9768,  9960, 13842,  2782,\n",
    "         5804,  6840, 11779, 15430,  5498, 11712, 14638, 13839, 10587,  7675,\n",
    "         3807,  4804,  2405, 14331,  6855,  8696,   691, 10110,  1662, 12189,\n",
    "         4337,  1407,  8180, 12151,  7950, 14786,  9190, 12440,  9858, 10916,\n",
    "           37, 13831,  2187,  9304, 14499,  3818, 10219,  2120,   940, 12739,\n",
    "        15805,  5615,  7988,  7237,  6098,  2239,  6606,  2314,  4022, 14229,\n",
    "         6646, 10719,  7381,  8680,  1436, 10023, 14613,  3863, 15777, 12609]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovered diff: -4.41883659362793\n"
     ]
    }
   ],
   "source": [
    "logits = run_with_saes_latent_op_patch_mean(clean_tokens, filtered_ids=filtered_ids, model=model, saes=[sae21],mean_cache=corr_sae_cache_means,dict_feats=dict_feats)\n",
    "mean_diff = logit_diff_fn(logits)\n",
    "print(f\"recovered diff: {mean_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06935815828908479"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-4.41883659362793 + 4.756180763244629 )/(0.44496282+4.418836593627)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested String:Int Dictionary Key Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> age = {'Lucas': {'Grace': 16, 'Rob': 15}, 'Sophia': {'Bob': 14, 'James': 15}}\n",
      ">>> var = age[\"Sophia\"][\"Rob\"]\n",
      "\n",
      "clean_diff: 2.3436012268066406\n",
      "corr_diff: -4.374980449676514\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Expanding the name pool with a larger set of names\n",
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\"\n",
    "]\n",
    "\n",
    "for name in extended_name_pool:\n",
    "    assert len(model.tokenizer.encode(name)) == 2, f\"Name {name} has more than 1 token\"\n",
    "# Function to generate the dataset for Level 2 - nested dictionaries\n",
    "# Function to generate the dataset for Level 2 - nested dictionaries with unique inner keys per outer key\n",
    "def generate_nested_dict_dataset_unique_inner_keys(name_pool, num_samples=5):\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select 5 outer keys\n",
    "        outer_keys = random.sample(name_pool, 2)\n",
    "        \n",
    "        # Create a nested dictionary where each outer key has a unique set of inner keys\n",
    "        nested_dict = {\n",
    "            outer_key: {inner_key: random.randint(10, 19) for inner_key in random.sample(name_pool, 2)}\n",
    "            for outer_key in outer_keys\n",
    "        }\n",
    "        \n",
    "        # Create a correct example\n",
    "        correct_outer_key = random.choice(list(nested_dict.keys()))\n",
    "        correct_inner_key = random.choice(list(nested_dict[correct_outer_key].keys()))\n",
    "        correct_prompt = (\n",
    "            f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f'>>> age = {nested_dict}\\n>>> var = age[\"{correct_outer_key}\"][\"{correct_inner_key}\"]\\n'\n",
    "        )\n",
    "        correct_response = nested_dict[correct_outer_key][correct_inner_key]\n",
    "        correct_token = str(correct_response)[0]\n",
    "        \n",
    "        # Create an incorrect example with a non-existent inner key for an existing outer key\n",
    "        incorrect_outer_key = correct_outer_key\n",
    "        incorrect_inner_key = random.choice(\n",
    "            [key for key in name_pool if key not in nested_dict[incorrect_outer_key]]\n",
    "        )\n",
    "        incorrect_prompt = (\n",
    "            f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f'>>> age = {nested_dict}\\n>>> var = age[\"{incorrect_outer_key}\"][\"{incorrect_inner_key}\"]\\n'\n",
    "        )\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "        \n",
    "        # Append the pair of correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# Generate the nested dictionary dataset\n",
    "json_dataset = generate_nested_dict_dataset_unique_inner_keys(extended_name_pool, num_samples=100)\n",
    "\n",
    "print(json_dataset[0]['error']['prompt'])\n",
    "\n",
    "# %%\n",
    "clean_prompts = []\n",
    "corr_prompts = []\n",
    "\n",
    "answer_token = model.to_single_token(\">>>\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in json_dataset[:50]:\n",
    "    corr_prompts.append(item[\"correct\"][\"prompt\"])\n",
    "    clean_prompts.append(item[\"error\"][\"prompt\"])\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompts)\n",
    "corr_tokens = model.to_tokens(corr_prompts)\n",
    "\n",
    "# %%\n",
    "def logit_diff_fn(logits):\n",
    "    err = logits[:, -1, traceback_token]\n",
    "    no_err = logits[:, -1, answer_token]\n",
    "    return (err - no_err).mean()\n",
    "\n",
    "# Disable gradients for all parameters\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "# # Compute logits for clean and corrupted samples\n",
    "logits = model(clean_tokens)\n",
    "clean_diff = logit_diff_fn(logits)\n",
    "\n",
    "logits = model(corr_tokens)\n",
    "corr_diff = logit_diff_fn(logits)\n",
    "\n",
    "print(f\"clean_diff: {clean_diff}\")\n",
    "print(f\"corr_diff: {corr_diff}\")\n",
    "\n",
    "# # Cleanup\n",
    "del logits\n",
    "cleanup_cuda()\n",
    "\n",
    "\n",
    "# # Define error type metric\n",
    "def _err_type_metric(logits, clean_logit_diff, corr_logit_diff):\n",
    "    patched_logit_diff = logit_diff_fn(logits)\n",
    "    return (patched_logit_diff - corr_logit_diff) / (clean_logit_diff - corr_logit_diff)\n",
    "\n",
    "err_metric_denoising = partial(_err_type_metric, clean_logit_diff=clean_diff, corr_logit_diff=corr_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 27 13:30:25 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:84:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             70W /  400W |   43469MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   4177677      C   ...u/.conda/envs/finetuning/bin/python      43460MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Value: tensor(1., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Clean Activations Cached: 1\n",
      "Corrupted Value: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Corrupted Activations Cached: 1\n",
      "Corrupted Gradients Cached: 1\n"
     ]
    }
   ],
   "source": [
    "clean_value, clean_cache, _ = get_cache_fwd_and_bwd(model, clean_tokens, err_metric_denoising, sae21)\n",
    "print(\"Clean Value:\", clean_value)\n",
    "print(\"Clean Activations Cached:\", len(clean_cache))\n",
    "# print(\"Clean Gradients Cached:\", len(clean_grad_cache))\n",
    "\n",
    "corrupted_value, corrupted_cache, corrupted_grad_cache = get_cache_fwd_and_bwd(model, corr_tokens, err_metric_denoising, sae21)\n",
    "print(\"Corrupted Value:\", corrupted_value)\n",
    "print(\"Corrupted Activations Cached:\", len(corrupted_cache))\n",
    "print(\"Corrupted Gradients Cached:\", len(corrupted_grad_cache))\n",
    "\n",
    "# # Cleanup\n",
    "del clean_value, corrupted_value\n",
    "cleanup_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 67, 16384]) torch.Size([50, 67, 16384])\n",
      "torch.Size([50, 67, 16384])\n"
     ]
    }
   ],
   "source": [
    "sae_acts = sae21.encode(clean_cache['blocks.21.hook_resid_post'][:, 1:, :])\n",
    "sae_acts_corr = sae21.encode(corrupted_cache['blocks.21.hook_resid_post'][:, 1:, :])\n",
    "print(sae_acts.shape, sae_acts_corr.shape)\n",
    "\n",
    "sae_grad_cache = torch.einsum('bij,kj->bik', corrupted_grad_cache['blocks.21.hook_resid_post'][:, 1:, :], sae21.W_dec)\n",
    "print(sae_grad_cache.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3237,  9585, 11219,  8535,  8156,  3271,  6290, 10711,  8233, 13756,\n",
      "        13163,  4766, 15430,  9858,  2405, 15571,  3395,   430, 10160,  8696,\n",
      "         1662, 12189, 14499, 13842,  5795, 13831,  9654,  4337, 15777, 10350,\n",
      "        14598, 11725,  3573,  3925,  6098, 10219,  5394, 11095,  9960, 11712,\n",
      "         5615, 14638, 11686,  2376,  7641,  8556, 11213,   111,  5478,  7892,\n",
      "         6353, 11774,  7674,  6138, 12223, 12813, 14605,   846,  4541, 13218,\n",
      "         7159,  3104,     0,     1,     2,     3,     4,     5,     6,     7,\n",
      "            8,     9,    10,    11,    12,    13,    14,    15,    16,    17,\n",
      "           18,    19,    20,    21,    22,    23,    24,    25,    26,    27,\n",
      "           28,    29,    30,    31,    32,    33,    34,    35,    36,    37],\n",
      "       device='cuda:0') tensor([ 7.7622e-02,  3.7153e-02,  2.2978e-02,  2.2002e-02,  2.0766e-02,\n",
      "         1.9949e-02,  1.7510e-02,  1.7235e-02,  1.5683e-02,  1.3382e-02,\n",
      "         1.0201e-02,  9.2319e-03,  8.8157e-03,  8.7468e-03, -7.5610e-03,\n",
      "         6.3876e-03,  5.4108e-03,  4.6909e-03,  4.6748e-03,  3.4279e-03,\n",
      "         3.2791e-03,  2.8019e-03,  2.6603e-03,  2.5509e-03,  2.3523e-03,\n",
      "        -2.0794e-03,  1.8373e-03,  1.7370e-03,  1.7250e-03,  1.6316e-03,\n",
      "        -1.5491e-03, -1.4068e-03, -1.3451e-03,  1.2360e-03, -1.1794e-03,\n",
      "         1.1440e-03,  1.1100e-03, -1.0885e-03, -1.0246e-03,  7.8210e-04,\n",
      "         6.9502e-04, -5.1642e-04,  4.1132e-04,  3.9270e-04, -3.9164e-04,\n",
      "         3.4096e-04,  3.1529e-04, -2.9034e-04, -2.0092e-04,  1.8753e-04,\n",
      "         1.7987e-04,  1.5936e-04,  1.5795e-04,  1.2232e-04, -1.1915e-04,\n",
      "        -1.1669e-04,  1.0136e-04, -9.1866e-05,  6.9027e-05, -6.6726e-05,\n",
      "         5.1291e-05,  8.7569e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "K = 100\n",
    "# Residual attribution calculation only for the selected positions\n",
    "residual_attr_final = einops.reduce(\n",
    "    sae_grad_cache[:, -1, :] * (sae_acts[:, -1, :] - sae_acts_corr[:, -1, :]),\n",
    "    \"batch n_features -> n_features\",\n",
    "    \"sum\",\n",
    ")\n",
    "# Get the top K features based on the absolute values\n",
    "abs_residual_attr_final = torch.abs(residual_attr_final)\n",
    "top_feats = torch.topk(abs_residual_attr_final, K)\n",
    "top_indices = top_feats.indices\n",
    "top_values = residual_attr_final[top_indices] \n",
    "print(top_indices, top_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Alphabet:Alphabet Dictionary Key Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> mapping = {'i': 's', 'd': 'a', 'x': 'n'}\n",
      ">>> var = mapping[\"e\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Function to generate Level 1 dataset\n",
    "def generate_level_1_dataset(num_samples=5):\n",
    "    dataset = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Generate a dictionary of the form {'a': 'b', 'c': 'd', 'e': 'f'}\n",
    "        keys = random.sample(string.ascii_lowercase, 3)\n",
    "        values = random.sample([ch for ch in string.ascii_lowercase if ch not in keys], 3)\n",
    "        alphabet_dict = dict(zip(keys, values))\n",
    "\n",
    "        # Create a correct example\n",
    "        correct_key = random.choice(keys)\n",
    "        correct_prompt = (\n",
    "            f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f'>>> mapping = {alphabet_dict}\\n>>> var = mapping[\"{correct_key}\"]\\n'\n",
    "        )\n",
    "        correct_response = alphabet_dict[correct_key]\n",
    "        correct_token = str(correct_response)[0]\n",
    "\n",
    "        # Create an incorrect example (key neither in keys nor values)\n",
    "        non_key_non_value = random.choice(\n",
    "            [ch for ch in string.ascii_lowercase if ch not in keys and ch not in values]\n",
    "        )\n",
    "        incorrect_prompt = (\n",
    "            f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f'>>> mapping = {alphabet_dict}\\n>>> var = mapping[\"{non_key_non_value}\"]\\n'\n",
    "        )\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "\n",
    "        # Append both correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate the datasets\n",
    "level_1_dataset = generate_level_1_dataset(num_samples=100)\n",
    "print(level_1_dataset[0]['error']['prompt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_diff: 1.7985607385635376\n",
      "corr_diff: -3.763065814971924\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "clean_prompts = []\n",
    "corr_prompts = []\n",
    "\n",
    "answer_token = model.to_single_token(\">>>\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in level_1_dataset[:50]:\n",
    "    corr_prompts.append(item[\"correct\"][\"prompt\"])\n",
    "    clean_prompts.append(item[\"error\"][\"prompt\"])\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompts)\n",
    "corr_tokens = model.to_tokens(corr_prompts)\n",
    "\n",
    "# %%\n",
    "def logit_diff_fn(logits):\n",
    "    err = logits[:, -1, traceback_token]\n",
    "    no_err = logits[:, -1, answer_token]\n",
    "    return (err - no_err).mean()\n",
    "\n",
    "# Disable gradients for all parameters\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "# # Compute logits for clean and corrupted samples\n",
    "logits = model(clean_tokens)\n",
    "clean_diff = logit_diff_fn(logits)\n",
    "\n",
    "logits = model(corr_tokens)\n",
    "corr_diff = logit_diff_fn(logits)\n",
    "\n",
    "print(f\"clean_diff: {clean_diff}\")\n",
    "print(f\"corr_diff: {corr_diff}\")\n",
    "\n",
    "# # Cleanup\n",
    "del logits\n",
    "cleanup_cuda()\n",
    "\n",
    "# # Define error type metric\n",
    "def _err_type_metric(logits, clean_logit_diff, corr_logit_diff):\n",
    "    patched_logit_diff = logit_diff_fn(logits)\n",
    "    return (patched_logit_diff - corr_logit_diff) / (clean_logit_diff - corr_logit_diff)\n",
    "\n",
    "err_metric_denoising = partial(_err_type_metric, clean_logit_diff=clean_diff, corr_logit_diff=corr_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Alphabet:Alphabet Dictionary Key Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> mapping = {'c': 'i', 'k': 'a', 't': 'z'}\n",
      ">>> var = mapping[\"i\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate Level 2 dataset\n",
    "def generate_level_2_dataset(num_samples=5):\n",
    "    dataset = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Generate a dictionary of the form {'a': 'b', 'c': 'd', 'e': 'f'}\n",
    "        keys = random.sample(string.ascii_lowercase, 3)\n",
    "        values = random.sample([ch for ch in string.ascii_lowercase if ch not in keys], 3)\n",
    "        alphabet_dict = dict(zip(keys, values))\n",
    "\n",
    "        # Create a correct example\n",
    "        correct_key = random.choice(keys)\n",
    "        correct_prompt = (\n",
    "            f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f'>>> mapping = {alphabet_dict}\\n>>> var = mapping[\"{correct_key}\"]\\n'\n",
    "        )\n",
    "        correct_response = alphabet_dict[correct_key]\n",
    "        correct_token = str(correct_response)[0]\n",
    "\n",
    "        # Create an incorrect example (key not in keys but IS in values)\n",
    "        non_key_but_value = random.choice(values)\n",
    "        incorrect_prompt = (\n",
    "            f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f'>>> mapping = {alphabet_dict}\\n>>> var = mapping[\"{non_key_but_value}\"]\\n'\n",
    "        )\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "\n",
    "        # Append both correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "level_2_dataset = generate_level_2_dataset(num_samples=100)\n",
    "print(level_2_dataset[0]['error']['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_diff: -1.2145028114318848\n",
      "corr_diff: -3.7032835483551025\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "clean_prompts = []\n",
    "corr_prompts = []\n",
    "\n",
    "answer_token = model.to_single_token(\">>>\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in level_2_dataset[:50]:\n",
    "    corr_prompts.append(item[\"correct\"][\"prompt\"])\n",
    "    clean_prompts.append(item[\"error\"][\"prompt\"])\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompts)\n",
    "corr_tokens = model.to_tokens(corr_prompts)\n",
    "\n",
    "# %%\n",
    "def logit_diff_fn(logits):\n",
    "    err = logits[:, -1, traceback_token]\n",
    "    no_err = logits[:, -1, answer_token]\n",
    "    return (err - no_err).mean()\n",
    "\n",
    "# Disable gradients for all parameters\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "# # Compute logits for clean and corrupted samples\n",
    "logits = model(clean_tokens)\n",
    "clean_diff = logit_diff_fn(logits)\n",
    "\n",
    "logits = model(corr_tokens)\n",
    "corr_diff = logit_diff_fn(logits)\n",
    "\n",
    "print(f\"clean_diff: {clean_diff}\")\n",
    "print(f\"corr_diff: {corr_diff}\")\n",
    "\n",
    "# # Cleanup\n",
    "del logits\n",
    "cleanup_cuda()\n",
    "\n",
    "# # Define error type metric\n",
    "def _err_type_metric(logits, clean_logit_diff, corr_logit_diff):\n",
    "    patched_logit_diff = logit_diff_fn(logits)\n",
    "    return (patched_logit_diff - corr_logit_diff) / (clean_logit_diff - corr_logit_diff)\n",
    "\n",
    "err_metric_denoising = partial(_err_type_metric, clean_logit_diff=clean_diff, corr_logit_diff=corr_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU: 0.5873015873015873\n"
     ]
    }
   ],
   "source": [
    "simple = [ 3237,  3271,  8156,  8233, 11219, 10711,  9585,  8535,  5795, 13756,\n",
    "         3925,   430, 10160,  6290, 11774,  9654,  3377, 13275, 13163,   111,\n",
    "         8311, 15795, 13862, 10350,  7892, 13218, 10618, 14598, 12223,  4766,\n",
    "         5577,  3395, 11725, 14495, 15571,  7482,  9768,  9960, 13842,  2782,\n",
    "         5804,  6840, 11779, 15430,  5498, 11712, 14638, 13839, 10587,  7675,\n",
    "         3807,  4804,  2405, 14331,  6855,  8696,   691, 10110,  1662, 12189,\n",
    "         4337,  1407,  8180, 12151,  7950, 14786,  9190, 12440,  9858, 10916,\n",
    "           37, 13831,  2187,  9304, 14499,  3818, 10219,  2120,   940, 12739,\n",
    "        15805,  5615,  7988,  7237,  6098,  2239,  6606,  2314,  4022, 14229,\n",
    "         6646, 10719,  7381,  8680,  1436, 10023, 14613,  3863, 15777, 12609]\n",
    "nested = [ 3237, 11219,  9585,  3271,  8535,  8233,  8156,  6290, 10711, 13756,\n",
    "         3395, 13163, 13275,  4766, 15430,  9858,  6855, 10160, 11774,   430,\n",
    "        14495, 15795, 13862,  2405, 15571,  2782, 10618, 13218,  4022, 12223,\n",
    "         8311,  3925,  6606,  3807, 14499,  8696,  1662,  7950,  4804,  9654,\n",
    "         9768, 11725, 13842, 12189,  5804,    37,  5795,  5870, 16071,  7237,\n",
    "         6840, 14229,   691, 13831, 11083,  4337,  2244,  5577, 15777, 12739,\n",
    "        10350, 12440, 16045,  1815, 14605, 10219, 13839,  3573, 14638,  8498,\n",
    "         5394, 11095, 13359, 14943,  9960, 14598,  6602, 10719,  8180,  9662,\n",
    "         4314,  7159, 14077, 11779, 12151, 11712, 12531,  3841,  5498,  1620,\n",
    "        11112,  5615, 10110,   524, 11609, 10023,  6098,  5875,  6503,  7482]\n",
    "\n",
    "def calculate_iou(list1, list2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two lists of integers.\n",
    "\n",
    "    Args:\n",
    "        list1 (list): First list of integers.\n",
    "        list2 (list): Second list of integers.\n",
    "\n",
    "    Returns:\n",
    "        float: IoU value.\n",
    "    \"\"\"\n",
    "    # Convert lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    \n",
    "    # Compute IoU\n",
    "    iou = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "    return iou\n",
    "\n",
    "iou = calculate_iou(simple, nested)\n",
    "print(f\"IoU: {iou}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
