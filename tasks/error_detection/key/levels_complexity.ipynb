{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6582c1634ad40a0859dd04b6cac866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "os.chdir(\"/work/pi_jensen_umass_edu/jnainani_umass_edu/CircuitAnalysisSAEs\")\n",
    "import json\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from circ4latents import data_gen\n",
    "from functools import partial\n",
    "import einops\n",
    "\n",
    "# Function to manage CUDA memory and clean up\n",
    "def cleanup_cuda():\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "# cleanup_cuda()\n",
    "# Load the config\n",
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-9b\", device=device, cache_dir=hf_cache)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae21 = SAE.from_pretrained(release=\"gemma-scope-9b-pt-res\", sae_id=f\"layer_21/width_16k/average_l0_36\", device=device)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', '>>>', ' my', '_', 'dict', ' =', ' {\"', 'a', '\":', ' ', '1', ',', ' \"', 'b', '\":', ' ', '2', '}', '\\n', '>>>', ' var', ' =', ' my', '_', 'dict', '[\"', 'a', '\"]', '\\n']\n",
      "Tokenized answer: [' ', '1']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.16</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.02</span><span style=\"font-weight: bold\">% Token: | |</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m-1.16\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1m% Token: | |\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit:  7.59 Prob: 98.11% Token: |>>>|\n",
      "Top 1th token. Logit:  2.14 Prob:  0.42% Token: |Traceback|\n",
      "Top 2th token. Logit:  1.77 Prob:  0.29% Token: |1|\n",
      "Top 3th token. Logit:  1.00 Prob:  0.13% Token: |#|\n",
      "Top 4th token. Logit:  0.75 Prob:  0.10% Token: |...|\n",
      "Top 5th token. Logit:  0.55 Prob:  0.09% Token: |2|\n",
      "Top 6th token. Logit:  0.48 Prob:  0.08% Token: |var|\n",
      "Top 7th token. Logit:  0.43 Prob:  0.08% Token: |<eos>|\n",
      "Top 8th token. Logit: -0.07 Prob:  0.05% Token: |>>|\n",
      "Top 9th token. Logit: -0.22 Prob:  0.04% Token: | >>>|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.75</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.78</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m24.75\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m23.78\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 25.75 Prob: 64.97% Token: |\n",
      "|\n",
      "Top 1th token. Logit: 24.75 Prob: 23.78% Token: |1|\n",
      "Top 2th token. Logit: 23.44 Prob:  6.42% Token: |\n",
      "\n",
      "|\n",
      "Top 3th token. Logit: 22.35 Prob:  2.16% Token: |2|\n",
      "Top 4th token. Logit: 20.85 Prob:  0.48% Token: |\n",
      "\n",
      "\n",
      "|\n",
      "Top 5th token. Logit: 20.67 Prob:  0.40% Token: |3|\n",
      "Top 6th token. Logit: 20.16 Prob:  0.24% Token: |\t|\n",
      "Top 7th token. Logit: 20.13 Prob:  0.24% Token: |5|\n",
      "Top 8th token. Logit: 19.71 Prob:  0.15% Token: |4|\n",
      "Top 9th token. Logit: 19.69 Prob:  0.15% Token: |0|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' '</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' '\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'1'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "prompt = \"\"\">>> my_dict = {\"a\": 1, \"b\": 2}\n",
    ">>> var = my_dict[\"a\"]\n",
    "\"\"\"\n",
    "test_prompt(prompt, \"1\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Key Error Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Error Detection with Duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_diff: 3.590031623840332\n",
      "corr_diff: -4.768364429473877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Expanding the name pool with a larger set of names\n",
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\"\n",
    "]\n",
    "\n",
    "for name in extended_name_pool:\n",
    "    assert len(model.tokenizer.encode(name)) == 2, f\"Name {name} has more than 1 token\"\n",
    "\n",
    "# Function to generate the dataset with correct and incorrect keying into dictionaries\n",
    "def generate_extended_dataset(name_pool, num_samples=5):\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select 5 names from the pool\n",
    "        selected_names = random.sample(name_pool, 5)\n",
    "        # Assign random ages to the selected names\n",
    "        age_dict = {name: random.randint(10, 19) for name in selected_names}\n",
    "        \n",
    "        # Create a correct example\n",
    "        correct_name = random.choice(list(age_dict.keys()))\n",
    "        correct_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> var = age[\"{correct_name}\"]\\n'\n",
    "        correct_response = age_dict[correct_name]\n",
    "        correct_token = str(correct_response)[0]\n",
    "        \n",
    "        # Create an incorrect example with a name not in the dictionary\n",
    "        incorrect_name = random.choice([name for name in name_pool if name not in age_dict])\n",
    "        incorrect_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> var = age[\"{incorrect_name}\"]\\n'\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "        \n",
    "        # Append the pair of correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# Generate the extended dataset\n",
    "json_dataset = generate_extended_dataset(extended_name_pool, num_samples=100)\n",
    "\n",
    "# Output the JSON structure\n",
    "\n",
    "# %%\n",
    "clean_prompts = []\n",
    "corr_prompts = []\n",
    "\n",
    "answer_token = model.to_single_token(\">>>\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in json_dataset[:50]:\n",
    "    corr_prompts.append(item[\"correct\"][\"prompt\"])\n",
    "    clean_prompts.append(item[\"error\"][\"prompt\"])\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompts)\n",
    "corr_tokens = model.to_tokens(corr_prompts)\n",
    "\n",
    "# %%\n",
    "def logit_diff_fn(logits):\n",
    "    err = logits[:, -1, traceback_token]\n",
    "    no_err = logits[:, -1, answer_token]\n",
    "    return (err - no_err).mean()\n",
    "\n",
    "# Disable gradients for all parameters\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "# # Compute logits for clean and corrupted samples\n",
    "logits = model(clean_tokens)\n",
    "clean_diff = logit_diff_fn(logits)\n",
    "\n",
    "logits = model(corr_tokens)\n",
    "corr_diff = logit_diff_fn(logits)\n",
    "\n",
    "print(f\"clean_diff: {clean_diff}\")\n",
    "print(f\"corr_diff: {corr_diff}\")\n",
    "\n",
    "# # Cleanup\n",
    "del logits\n",
    "cleanup_cuda()\n",
    "\n",
    "# # Define error type metric\n",
    "def _err_type_metric(logits, clean_logit_diff, corr_logit_diff):\n",
    "    patched_logit_diff = logit_diff_fn(logits)\n",
    "    return (patched_logit_diff - corr_logit_diff) / (clean_logit_diff - corr_logit_diff)\n",
    "\n",
    "err_metric_denoising = partial(_err_type_metric, clean_logit_diff=clean_diff, corr_logit_diff=corr_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> mapping = {'Charlie': 'Grace', 'Ava': 'Chris', 'Sally': 'Isabella', 'Chris': 'Chris', 'Grace': 'Noah'}\n",
      ">>> var = mapping[\"Sally\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Expanding the name pool with a larger set of names\n",
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\"\n",
    "]\n",
    "\n",
    "for name in extended_name_pool:\n",
    "    assert len(model.tokenizer.encode(name)) == 2, f\"Name {name} has more than 1 token\"\n",
    "\n",
    "# Function to generate the dataset for Level 3 - dicts with duplicate\n",
    "def generate_duplicate_key_dataset(name_pool, num_samples=5):\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select 5 names from the pool\n",
    "        selected_names = random.sample(name_pool, 5)\n",
    "        # Create a dictionary where some values are also keys\n",
    "        value_keys = random.sample(selected_names, 2)  # Choose 2 names to act as both keys and values\n",
    "        random_values = random.choices(name_pool, k=5)\n",
    "        age_dict = {key: value_keys[i % len(value_keys)] if i < len(value_keys) else random_values[i]\n",
    "                    for i, key in enumerate(selected_names)}\n",
    "\n",
    "        # Create a correct example\n",
    "        correct_key = random.choice(list(age_dict.keys()))\n",
    "        correct_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> mapping = {age_dict}\\n>>> var = mapping[\"{correct_key}\"]\\n'\n",
    "        correct_response = age_dict[correct_key]\n",
    "        correct_token = model.to_single_token(str(correct_response))\n",
    "\n",
    "        # Create an incorrect example by attempting to access a value as if it's a key\n",
    "        incorrect_value = random.choice([value for value in age_dict.values() if value not in age_dict])\n",
    "        incorrect_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> mapping = {age_dict}\\n>>> var = mapping[\"{incorrect_value}\"]\\n'\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "        \n",
    "        # Append the pair of correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# Generate the duplicate key dataset\n",
    "json_dataset = generate_duplicate_key_dataset(extended_name_pool, num_samples=10)\n",
    "print(json_dataset[0]['correct']['prompt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> mapping = {'Charlie': 'Grace', 'Ava': 'Chris', 'Sally': 'Isabella', 'Chris': 'Chris', 'Grace': 'Noah'}\n",
      ">>> var = mapping[\"Isabella\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(json_dataset[0]['error']['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Type', ' \"', 'help', '\",', ' \"', 'copyright', '\",', ' \"', 'credits', '\"', ' or', ' \"', 'license', '\"', ' for', ' more', ' information', '.', '\\n', '>>>', ' mapping', ' =', \" {'\", 'Charlie', \"':\", \" '\", 'Grace', \"',\", \" '\", 'Ava', \"':\", \" '\", 'Chris', \"',\", \" '\", 'Sally', \"':\", \" '\", 'Isabella', \"',\", \" '\", 'Grace', \"':\", \" '\", 'Noah', \"'}\", '\\n', '>>>', ' print', '(', 'mapping', '[\"', 'Isabella', '\"])', '\\n']\n",
      "Tokenized answer: ['Traceback']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.29</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.61</span><span style=\"font-weight: bold\">% Token: |Traceback|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m25.29\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m2.61\u001b[0m\u001b[1m% Token: |Traceback|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 27.90 Prob: 35.36% Token: |Sally|\n",
      "Top 1th token. Logit: 27.34 Prob: 20.17% Token: |Grace|\n",
      "Top 2th token. Logit: 26.75 Prob: 11.27% Token: |Noah|\n",
      "Top 3th token. Logit: 26.36 Prob:  7.60% Token: |>>>|\n",
      "Top 4th token. Logit: 25.69 Prob:  3.90% Token: |Ava|\n",
      "Top 5th token. Logit: 25.63 Prob:  3.64% Token: |Isabella|\n",
      "Top 6th token. Logit: 25.29 Prob:  2.61% Token: |Chris|\n",
      "Top 7th token. Logit: 25.29 Prob:  2.61% Token: |Traceback|\n",
      "Top 8th token. Logit: 25.11 Prob:  2.18% Token: |Charlie|\n",
      "Top 9th token. Logit: 24.68 Prob:  1.42% Token: |'|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Traceback'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'Traceback'\u001b[0m, \u001b[1;36m7\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "prompt = \"\"\"Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> mapping = {'Charlie': 'Grace', 'Ava': 'Chris', 'Sally': 'Isabella', 'Grace': 'Noah'}\n",
    ">>> print(mapping[\"Isabella\"])\n",
    "\"\"\"\n",
    "test_prompt(prompt, \"Traceback\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Type', ' \"', 'help', '\",', ' \"', 'copyright', '\",', ' \"', 'credits', '\"', ' or', ' \"', 'license', '\"', ' for', ' more', ' information', '.', '\\n', '>>>', ' mapping', ' =', \" {'\", 'a', \"':\", \" '\", 'b', \"',\", \" '\", 'c', \"':\", \" '\", 'd', \"',\", \" '\", 'e', \"':\", \" '\", 'f', \"'}\", '\\n', '>>>', ' print', '(', 'mapping', '[\"', 'f', '\"])', '\\n']\n",
      "Tokenized answer: ['Traceback']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.02</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.37</span><span style=\"font-weight: bold\">% Token: |Traceback|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m27.02\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m50.37\u001b[0m\u001b[1m% Token: |Traceback|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 27.02 Prob: 50.37% Token: |Traceback|\n",
      "Top 1th token. Logit: 25.30 Prob:  9.04% Token: |e|\n",
      "Top 2th token. Logit: 25.22 Prob:  8.39% Token: |>>>|\n",
      "Top 3th token. Logit: 24.76 Prob:  5.24% Token: |Syntax|\n",
      "Top 4th token. Logit: 24.47 Prob:  3.94% Token: |f|\n",
      "Top 5th token. Logit: 24.17 Prob:  2.92% Token: |None|\n",
      "Top 6th token. Logit: 23.95 Prob:  2.35% Token: |b|\n",
      "Top 7th token. Logit: 23.88 Prob:  2.19% Token: |d|\n",
      "Top 8th token. Logit: 23.47 Prob:  1.46% Token: |a|\n",
      "Top 9th token. Logit: 23.28 Prob:  1.20% Token: |'|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Traceback'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'Traceback'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "prompt = \"\"\"Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> mapping = {'a': 'b', 'c': 'd', 'e': 'f'}\n",
    ">>> print(mapping[\"f\"])\n",
    "\"\"\"\n",
    "test_prompt(prompt, \"Traceback\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Type', ' \"', 'help', '\",', ' \"', 'copyright', '\",', ' \"', 'credits', '\"', ' or', ' \"', 'license', '\"', ' for', ' more', ' information', '.', '\\n', '>>>', ' my', '_', 'dict', ' =', \" {'\", 'Charlie', \"':\", \" '\", 'Grace', \"',\", \" '\", 'Ava', \"':\", \" '\", 'Chris', \"',\", \" '\", 'Sally', \"':\", \" '\", 'Isabella', \"',\", \" '\", 'Grace', \"':\", \" '\", 'Noah', \"'}\", '\\n', '>>>', ' for', ' key', ' in', ' [\"', 'Charlie', '\",', ' \"', 'Ava', '\",', ' \"', 'Isabella', '\"]:', '\\n', '...', '     ', 'print', '(', 'my', '_', 'dict', '[', 'key', '])', '\\n', '...', ' ', '\\n', 'Grace', '\\n', 'Chris', '\\n']\n",
      "Tokenized answer: ['Traceback']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.56</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.15</span><span style=\"font-weight: bold\">% Token: |Traceback|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m22.56\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1m% Token: |Traceback|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 28.63 Prob: 64.16% Token: |Noah|\n",
      "Top 1th token. Logit: 26.97 Prob: 12.28% Token: |Isabella|\n",
      "Top 2th token. Logit: 26.55 Prob:  8.05% Token: |Grace|\n",
      "Top 3th token. Logit: 26.08 Prob:  5.01% Token: |Sally|\n",
      "Top 4th token. Logit: 25.12 Prob:  1.93% Token: |>>>|\n",
      "Top 5th token. Logit: 25.01 Prob:  1.72% Token: |Chris|\n",
      "Top 6th token. Logit: 24.82 Prob:  1.43% Token: |None|\n",
      "Top 7th token. Logit: 23.97 Prob:  0.61% Token: |'|\n",
      "Top 8th token. Logit: 23.85 Prob:  0.54% Token: |No|\n",
      "Top 9th token. Logit: 23.69 Prob:  0.46% Token: |...|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Traceback'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'Traceback'\u001b[0m, \u001b[1;36m15\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "prompt = \"\"\"Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> my_dict = {'Charlie': 'Grace', 'Ava': 'Chris', 'Sally': 'Isabella', 'Grace': 'Noah'}\n",
    ">>> for key in [\"Charlie\", \"Ava\", \"Isabella\"]:\n",
    "...     print(my_dict[key])\n",
    "... \n",
    "Grace\n",
    "Chris\n",
    "\"\"\"\n",
    "test_prompt(prompt, \"Traceback\", model, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Isabella'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/work/pi_jensen_umass_edu/jnainani_umass_edu/CircuitAnalysisSAEs/tasks/error_detection/key/levels_complexity.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu021.unity.rc.umass.edu/work/pi_jensen_umass_edu/jnainani_umass_edu/CircuitAnalysisSAEs/tasks/error_detection/key/levels_complexity.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m mapping \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mCharlie\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mGrace\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAva\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mChris\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSally\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mIsabella\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGrace\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mNoah\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu021.unity.rc.umass.edu/work/pi_jensen_umass_edu/jnainani_umass_edu/CircuitAnalysisSAEs/tasks/error_detection/key/levels_complexity.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m var \u001b[39m=\u001b[39m mapping[\u001b[39m\"\u001b[39;49m\u001b[39mIsabella\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Isabella'"
     ]
    }
   ],
   "source": [
    "mapping = {'Charlie': 'Grace', 'Ava': 'Chris', 'Sally': 'Isabella', 'Grace': 'Noah'}\n",
    "var = mapping[\"Isabella\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> data = {'Grace': {'Emma': 14, 'Megan': 11}, 'Bob': {'Lucas': 19, 'Emma': 19}, 'Olivia': {'Isabella': 13, 'Sally': 14}}\n",
      ">>> var = data[\"Grace\"][\"Emma\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Expanding the name pool with a larger set of names\n",
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\"\n",
    "]\n",
    "\n",
    "for name in extended_name_pool:\n",
    "    assert len(model.tokenizer.encode(name)) == 2, f\"Name {name} has more than 1 token\"\n",
    "# Function to generate the dataset for Level 2 - nested dictionaries\n",
    "# Function to generate the dataset for Level 2 - nested dictionaries with unique inner keys per outer key\n",
    "def generate_nested_dict_dataset_unique_inner_keys(name_pool, num_samples=5):\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select 5 outer keys\n",
    "        outer_keys = random.sample(name_pool, 3)\n",
    "        \n",
    "        # Create a nested dictionary where each outer key has a unique set of inner keys\n",
    "        nested_dict = {\n",
    "            outer_key: {inner_key: random.randint(10, 19) for inner_key in random.sample(name_pool, 2)}\n",
    "            for outer_key in outer_keys\n",
    "        }\n",
    "        \n",
    "        # Create a correct example\n",
    "        correct_outer_key = random.choice(list(nested_dict.keys()))\n",
    "        correct_inner_key = random.choice(list(nested_dict[correct_outer_key].keys()))\n",
    "        correct_prompt = (\n",
    "            f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f'>>> data = {nested_dict}\\n>>> var = data[\"{correct_outer_key}\"][\"{correct_inner_key}\"]\\n'\n",
    "        )\n",
    "        correct_response = nested_dict[correct_outer_key][correct_inner_key]\n",
    "        correct_token = str(correct_response)[0]\n",
    "        \n",
    "        # Create an incorrect example with a non-existent inner key for an existing outer key\n",
    "        incorrect_outer_key = correct_outer_key\n",
    "        incorrect_inner_key = random.choice(\n",
    "            [key for key in name_pool if key not in nested_dict[incorrect_outer_key]]\n",
    "        )\n",
    "        incorrect_prompt = (\n",
    "            f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n'\n",
    "            f'>>> data = {nested_dict}\\n>>> var = data[\"{incorrect_outer_key}\"][\"{incorrect_inner_key}\"]\\n'\n",
    "        )\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "        \n",
    "        # Append the pair of correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# Generate the nested dictionary dataset\n",
    "json_dataset = generate_nested_dict_dataset_unique_inner_keys(extended_name_pool, num_samples=100)\n",
    "\n",
    "print(json_dataset[0]['correct']['prompt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
