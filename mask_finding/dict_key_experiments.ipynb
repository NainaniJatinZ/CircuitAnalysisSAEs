{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM, SAEs\n",
    "\n",
    "sae layers: [7, 14, 21, 28, 40]\n",
    "llm gemma 9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4135a2745440c795282db94b419094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-9b into HookedTransformer\n",
      "pad token id is 0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import circuitsvis as cv\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "# device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.set_default_device(device)\n",
    "# assert device == 'cuda', \"This notebook is not optimized for CPU\"\n",
    "\n",
    "import transformer_lens\n",
    "\n",
    "# Load a model\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gemma-2-9b\", device=\"cuda\")\n",
    "pad_token_id = model.tokenizer.pad_token_id\n",
    "print('pad token id is', pad_token_id)\n",
    "\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_device(device)\n",
    "assert device == 'cuda', \"This notebook is not optimized for CPU\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "def get_gemma_9b_sae_id(layer):\n",
    "    return f\"layer_{layer}/width_16k/canonical\"\n",
    "# sae_id = get_gemma_9b_sae_id(10)\n",
    "# sae, cfg_dict, sparsity = SAE.from_pretrained(release=\"gemma-scope-9b-pt-res-canonical\", sae_id=\"layer_10/width_16k/canonical\", device=device)\n",
    "# saes_to_load = [7, 14, 21, 40]\n",
    "saes_to_load = [14, 21]\n",
    "\n",
    "\n",
    "saes = []\n",
    "for sae_layer in saes_to_load:\n",
    "    sae, cfg_dict, sparsity = SAE.from_pretrained(release=\"gemma-scope-9b-pt-res-canonical\", sae_id=get_gemma_9b_sae_id(sae_layer), device=device)\n",
    "    saes.append(sae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sae in saes:\n",
    "    for param in sae.parameters():\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated version to return JSON with more names and structure for correct and incorrect keying examples\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Expanding the name pool with a larger set of names\n",
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\"\n",
    "]\n",
    "\n",
    "for name in extended_name_pool:\n",
    "    assert len(model.tokenizer.encode(name)) == 2, f\"Name {name} has more than 1 token\"\n",
    "\n",
    "# Function to generate the dataset with correct and incorrect keying into dictionaries\n",
    "def generate_extended_dataset(name_pool, num_samples=5):\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select 5 names from the pool\n",
    "        selected_names = random.sample(name_pool, 5)\n",
    "        # Assign random ages to the selected names\n",
    "        age_dict = {name: random.randint(10, 19) for name in selected_names}\n",
    "        \n",
    "        # Create a correct example\n",
    "        correct_name = random.choice(list(age_dict.keys()))\n",
    "        correct_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> age[\"{correct_name}\"]\\n'\n",
    "        correct_response = age_dict[correct_name]\n",
    "        correct_token = str(correct_response)[0]\n",
    "        \n",
    "        # Create an incorrect example with a name not in the dictionary\n",
    "        incorrect_name = random.choice([name for name in name_pool if name not in age_dict])\n",
    "        incorrect_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> age[\"{incorrect_name}\"]\\n'\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "        \n",
    "        # Append the pair of correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Generate the extended dataset\n",
    "json_dataset = generate_extended_dataset(extended_name_pool, num_samples=100_000)\n",
    "\n",
    "# Output the JSON structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': {'prompt': 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {\\'Grace\\': 19, \\'Rob\\': 10, \\'Liam\\': 11, \\'Alice\\': 16, \\'Benjamin\\': 14}\\n>>> age[\"Liam\"]\\n',\n",
       "  'response': 11,\n",
       "  'token': '1'},\n",
       " 'error': {'prompt': 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {\\'Grace\\': 19, \\'Rob\\': 10, \\'Liam\\': 11, \\'Alice\\': 16, \\'Benjamin\\': 14}\\n>>> age[\"Oliver\"]\\n',\n",
       "  'response': 'Traceback',\n",
       "  'token': 'Traceback'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 235285,   1154,   4506,      0,      0],\n",
       "        [     2, 235285,   1154,  11514,    578,   4506],\n",
       "        [     2,  39954,      0,      0,      0,      0],\n",
       "        [     2,  34371,      0,      0,      0,      0]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = model.to_tokens([\"I like pie\", \"I like cake and pie\", \"birds\", 'cats'])\n",
    "torch.count_nonzero(tokenized != pad_token_id, dim=-1)-1\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 1, 2, 3, 4], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensa = torch.tensor([1,2,3,])\n",
    "tesb = torch.tensor([1,2,3,4])\n",
    "torch.cat([tensa, tesb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDatasetBatch:\n",
    "    def __init__(self, dataset_items):\n",
    "        self.correct_batch = [item[\"correct\"] for item in dataset_items]\n",
    "        self.error_batch = [item[\"error\"] for item in dataset_items]\n",
    "        self.batch_size = len(self.correct_batch)\n",
    "        self.correct_token_idx = model.to_single_token(\"1\")\n",
    "        self.error_token_idx = model.to_single_token(\"Traceback\")\n",
    "\n",
    "        correct_tokenized = None\n",
    "        error_tokenized = None\n",
    "\n",
    "        correct_prompts = [example[\"prompt\"] for example in self.correct_batch]\n",
    "        error_prompts = [example[\"prompt\"] for example in self.error_batch]\n",
    "        assert len(correct_prompts) == len(error_prompts)\n",
    "        all_prompts = correct_prompts + error_prompts\n",
    "        all_tokenized = model.to_tokens(all_prompts)\n",
    "        last_non_pad_idxs = torch.count_nonzero(all_tokenized != pad_token_id, dim=-1) - 1\n",
    "\n",
    "\n",
    "        correct_tokenized = all_tokenized[:self.batch_size]\n",
    "        correct_answer_idxs = last_non_pad_idxs[:self.batch_size]\n",
    "        error_tokenized = all_tokenized[self.batch_size:]\n",
    "        error_answer_idxs = last_non_pad_idxs[self.batch_size:]\n",
    "\n",
    "        self.correct_tokenized = correct_tokenized\n",
    "        self.error_tokenized = error_tokenized\n",
    "        self.correct_answer_seq_idxs = correct_answer_idxs\n",
    "        self.error_answer_seq_idxs = error_answer_idxs\n",
    "\n",
    "        self.all_tokenized = all_tokenized\n",
    "        self.all_answer_seq_idxs = last_non_pad_idxs\n",
    "        self.all_prompts = all_prompts\n",
    "        # the tokens are: [correct prompt, correct prompt, ..., error prompt, error prompt, ...]\n",
    "        self.all_answers_tok_idxs = torch.cat(\n",
    "            [\n",
    "                torch.ones(self.batch_size, dtype=torch.int64)*self.correct_token_idx,\n",
    "                torch.ones(self.batch_size, dtype=torch.int64)*self.error_token_idx\n",
    "            ]\n",
    "            )\n",
    "        self.all_wrong_answers_tok_idxs = torch.cat(\n",
    "            [\n",
    "                torch.ones(self.batch_size, dtype=torch.int64)*self.error_token_idx,\n",
    "                torch.ones(self.batch_size, dtype=torch.int64)*self.correct_token_idx\n",
    "            ]\n",
    "            )\n",
    "    \n",
    "    def get_logit_diffs(self, clean_logits, error_logits):\n",
    "        # for the clean pass, get the logit for the \"1\" token and the \"Traceback\" token\n",
    "        correct_code_right_logits = clean_logits[torch.arange(self.batch_size), self.correct_answer_seq_idxs, self.correct_token_idx]\n",
    "        correct_code_wrong_logits = clean_logits[torch.arange(self.batch_size), self.correct_answer_seq_idxs, self.error_token_idx]\n",
    "        correct_logit_diffs = correct_code_right_logits - correct_code_wrong_logits\n",
    "\n",
    "        # for the error pass, get the logit for the Traceback token and the \"1\" token\n",
    "        error_code_right_logits = error_logits[torch.arange(self.batch_size), self.error_answer_seq_idxs, self.error_token_idx]\n",
    "        error_code_wrong_logits = error_logits[torch.arange(self.batch_size), self.error_answer_seq_idxs, self.correct_token_idx]\n",
    "        error_logit_diffs = error_code_right_logits - error_code_wrong_logits\n",
    "        return {\"correct_code_diff\": correct_logit_diffs, \"error_code_diff\": error_logit_diffs}\n",
    "        \n",
    "\n",
    "\n",
    "eval_dataset = ContrastiveDatasetBatch(json_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset = []\n",
    "simple_labels = []\n",
    "\n",
    "answer_token = model.to_single_token(\"1\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in json_dataset:\n",
    "    simple_dataset.append(item[\"correct\"][\"prompt\"])\n",
    "    simple_dataset.append(item[\"error\"][\"prompt\"])\n",
    "    simple_labels.append(answer_token)\n",
    "    simple_labels.append(traceback_token)\n",
    "\n",
    "\n",
    "simple_dataset = model.to_tokens(simple_dataset)\n",
    "simple_labels = torch.tensor(simple_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = torch.randperm(len(simple_dataset))\n",
    "simple_dataset = simple_dataset[permutation]\n",
    "simple_labels = simple_labels[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:'\n",
      "'\n",
      "Answer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Traceback'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"token:'{model.to_str_tokens(eval_dataset.all_tokenized[15])[eval_dataset.all_answer_seq_idxs[15]]}'\")\n",
    "print(\"Answer:\")\n",
    "model.to_string([eval_dataset.all_answers_tok_idxs[15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMask(nn.Module):\n",
    "    def __init__(self, shape, l1):\n",
    "        super().__init__()\n",
    "        self.mask = nn.Parameter(torch.ones(shape))\n",
    "        self.l1 = l1\n",
    "        self.max_temp = torch.tensor(1000.0)\n",
    "        self.sparsity_loss = None\n",
    "        self.ratio_trained = 1\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        temperature = self.max_temp ** self.ratio_trained\n",
    "        mask = torch.sigmoid(self.mask * temperature)\n",
    "        self.sparsity_loss = torch.abs(mask).sum() * self.l1\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "saes[0].mask = SparseMask(saes[0].cfg.d_sae, 1.0)\n",
    "saes[1].mask = SparseMask(saes[1].cfg.d_sae, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = model.tokenizer.bos_token_id\n",
    "\n",
    "\n",
    "\n",
    "def build_sae_hook_fn(sae, sequence, cache_grads=False, circuit_mask=None, use_mask=False):\n",
    "    # make the mask for the sequence\n",
    "    mask = torch.ones_like(sequence, dtype=torch.bool)\n",
    "    mask[sequence == pad_token_id] = False\n",
    "    mask[sequence == bos_token_id] = False # where mask is false, keep original\n",
    "    def sae_hook(value, hook):\n",
    "        # print(f\"sae {sae.cfg.hook_name} running at layer {hook.layer()}\")\n",
    "        feature_acts = sae.encode(value)\n",
    "        if cache_grads:\n",
    "            sae.feature_acts = feature_acts\n",
    "            sae.feature_acts.retain_grad()\n",
    "        \n",
    "        if use_mask:\n",
    "            feature_acts = sae.mask(feature_acts) \n",
    "\n",
    "        if circuit_mask is not None:\n",
    "            mask_method = circuit_mask['mask_method']\n",
    "            mask_indices = circuit_mask[sae.cfg.hook_name]\n",
    "            if mask_method == 'keep_only':\n",
    "                # any activations not in the mask are set to 0\n",
    "                expanded_circuit_mask = torch.zeros_like(feature_acts)\n",
    "                expanded_circuit_mask[:, :, mask_indices] = 1\n",
    "                feature_acts = feature_acts * expanded_circuit_mask\n",
    "            elif mask_method == 'zero_only':\n",
    "                feature_acts[:, :, mask_indices] = 0\n",
    "            else:\n",
    "                raise ValueError(f\"mask_method {mask_method} not recognized\")\n",
    "\n",
    "\n",
    "        out = sae.decode(feature_acts)\n",
    "        # choose out or value based on the mask\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(value)\n",
    "        value = torch.where(mask_expanded, out, value)\n",
    "        return value\n",
    "    return sae_hook\n",
    "\n",
    "\n",
    "    # def sae_hook_ablate(value, hook):\n",
    "    # feature_acts = sae.encode(value)\n",
    "    # # feature_acts[:, :, topsae_attr_indices] = 0\n",
    "    # out = sae.decode(feature_acts)\n",
    "    # return out\n",
    "\n",
    "\n",
    "def build_hooks_list(sequence, cache_sae_grads=False, circuit_mask=None, use_mask=False):\n",
    "    hooks = []\n",
    "    # blocks.0.hook_resid_pre\n",
    "    # # fake hook that adds zero so gradients propagate through the model\n",
    "    param = nn.Parameter(torch.tensor(0.0, requires_grad=True))\n",
    "    hooks.append(\n",
    "        (\n",
    "            \"blocks.0.hook_resid_pre\",\n",
    "            lambda value, hook: value + param,\n",
    "        )\n",
    "    )\n",
    "    for sae in saes:\n",
    "        hooks.append(\n",
    "            (\n",
    "            sae.cfg.hook_name,\n",
    "            build_sae_hook_fn(sae, sequence, cache_grads=cache_sae_grads, circuit_mask=circuit_mask, use_mask=use_mask),\n",
    "            )\n",
    "        )\n",
    "    return hooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_model_performance(logitfn):\n",
    "    baseline_dataset = ContrastiveDatasetBatch(json_dataset[0:10])\n",
    "    correct_logits = logitfn(baseline_dataset.correct_tokenized)\n",
    "    error_logits = logitfn(baseline_dataset.error_tokenized)\n",
    "    print(\"probability of predicting the correct age in the correct example\")\n",
    "    print(F.softmax(correct_logits[torch.arange(correct_logits.shape[0]), baseline_dataset.correct_answer_seq_idxs], dim=-1)[:, baseline_dataset.correct_token_idx].mean())\n",
    "    print(\"and of traceback in that example\")\n",
    "    print(F.softmax(correct_logits[torch.arange(correct_logits.shape[0]), baseline_dataset.correct_answer_seq_idxs], dim=-1)[:, baseline_dataset.error_token_idx].mean())\n",
    "    print(\"logit difference:\")\n",
    "    correct_logit_values = correct_logits[torch.arange(correct_logits.shape[0]), baseline_dataset.correct_answer_seq_idxs, baseline_dataset.correct_token_idx]\n",
    "    error_logit_values = correct_logits[torch.arange(correct_logits.shape[0]), baseline_dataset.correct_answer_seq_idxs, baseline_dataset.error_token_idx]\n",
    "    diff = correct_logit_values - error_logit_values\n",
    "    print(diff.mean())\n",
    "\n",
    "    print(\"probability of predicting the traceback in the error code example\")\n",
    "    print(F.softmax(error_logits[torch.arange(error_logits.shape[0]), baseline_dataset.error_answer_seq_idxs], dim=-1)[:, baseline_dataset.error_token_idx].mean())\n",
    "    print(\"and of the correct age in that example\")\n",
    "    print(F.softmax(error_logits[torch.arange(error_logits.shape[0]), baseline_dataset.error_answer_seq_idxs], dim=-1)[:, baseline_dataset.correct_token_idx].mean())    \n",
    "    print(\"logit difference:\")\n",
    "    correct_logit_values = error_logits[torch.arange(error_logits.shape[0]), baseline_dataset.error_answer_seq_idxs, baseline_dataset.error_token_idx]\n",
    "    error_logit_values = error_logits[torch.arange(error_logits.shape[0]), baseline_dataset.error_answer_seq_idxs, baseline_dataset.correct_token_idx]\n",
    "    diff = correct_logit_values - error_logit_values\n",
    "    print(diff.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_contrastive_difference(logitfn):\n",
    "    baseline_dataset = ContrastiveDatasetBatch(json_dataset[0:10])\n",
    "    all_tokenized = baseline_dataset.all_tokenized\n",
    "    all_answer_seq_idxs = baseline_dataset.all_answer_seq_idxs\n",
    "    all_answers_tok_idxs = baseline_dataset.all_answers_tok_idxs\n",
    "    all_wrong_answers_tok_idxs = baseline_dataset.all_wrong_answers_tok_idxs\n",
    "    all_prompts = baseline_dataset.all_prompts\n",
    "    \n",
    "    all_logits = logitfn(all_tokenized)\n",
    "    all_answer_logits = all_logits[torch.arange(all_logits.shape[0]), all_answer_seq_idxs]\n",
    "    all_answers_correct_logits = all_answer_logits[torch.arange(all_answer_logits.shape[0]), all_answers_tok_idxs]\n",
    "    all_answers_wrong_logits = all_answer_logits[torch.arange(all_answer_logits.shape[0]), all_wrong_answers_tok_idxs]\n",
    "    all_logit_diffs = all_answers_correct_logits - all_answers_wrong_logits\n",
    "    # 6.7 dif for correct, 1.8 for error\n",
    "    all_logit_diffs[0:baseline_dataset.batch_size] = all_logit_diffs[0:baseline_dataset.batch_size]/6.7\n",
    "    all_logit_diffs[baseline_dataset.batch_size:] = all_logit_diffs[baseline_dataset.batch_size:]/1.8\n",
    "    return all_logit_diffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE vs no SAE (sanity check and basic setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dataset = ContrastiveDatasetBatch(json_dataset[10:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of predicting the correct age in the correct example\n",
      "tensor(0.9827, device='cuda:0')\n",
      "and of traceback in that example\n",
      "tensor(0.0010, device='cuda:0')\n",
      "logit difference:\n",
      "tensor(7.0165, device='cuda:0')\n",
      "probability of predicting the traceback in the error code example\n",
      "tensor(0.9648, device='cuda:0')\n",
      "and of the correct age in that example\n",
      "tensor(0.0008, device='cuda:0')\n",
      "logit difference:\n",
      "tensor(7.1005, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def baseline_model_logit_fn(tokens):\n",
    "    correct_logits = model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=[\n",
    "            #(\n",
    "            #utils.get_act_name(\"pre\", 0), # v=attention out\n",
    "            #\"blocks.0.hook_mlp_out\",\n",
    "            #mlp_ablation_hook,\n",
    "            #),\n",
    "            ]\n",
    "        )\n",
    "    return correct_logits\n",
    "\n",
    "with torch.no_grad():\n",
    "    sanity_check_model_performance(baseline_model_logit_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_token_id\n",
    "bos_token_id = model.tokenizer.bos_token_id\n",
    "bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of predicting the correct age in the correct example\n",
      "tensor(0.9518, device='cuda:0')\n",
      "and of traceback in that example\n",
      "tensor(0.0048, device='cuda:0')\n",
      "logit difference:\n",
      "tensor(5.3536, device='cuda:0')\n",
      "probability of predicting the traceback in the error code example\n",
      "tensor(0.6051, device='cuda:0')\n",
      "and of the correct age in that example\n",
      "tensor(0.0273, device='cuda:0')\n",
      "logit difference:\n",
      "tensor(3.1331, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens)\n",
    "            )\n",
    "    sanity_check_model_performance(logitfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens)\n",
    "            )\n",
    "    logits = logitfn(baseline_dataset.error_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Traceback', '>>>', 'None']\n",
      "tensor([25.2613, 23.6651, 21.7949], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(21.5102, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = 4\n",
    "topk = torch.topk(logits[:, -1, :][test_idx], k=3)\n",
    "print(model.to_str_tokens(topk.indices))\n",
    "print(topk.values)\n",
    "logits[:, -1, :][test_idx][baseline_dataset.correct_token_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Logit Diff With Positive and Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7308, 0.8568, 0.7782, 0.7600, 0.8709, 0.8032, 0.8597, 0.7421, 0.7308,\n",
      "        0.8580, 1.5148, 1.7967, 1.8832, 1.5217, 1.9293, 1.7721, 1.8547, 1.7349,\n",
      "        2.0564, 1.3420], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens)\n",
    "            )\n",
    "    all_logit_diffs = all_contrastive_difference(logitfn)\n",
    "    print(all_logit_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitfn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, cache_sae_grads=True)\n",
    "        )\n",
    "\n",
    "all_logit_diffs = all_contrastive_difference(logitfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = all_logit_diffs.sum() * -1 # maximize the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-26.7789, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ablate = {\"mask_method\": \"keep_only\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = saes[0].cfg.d_sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for sae in saes:\n",
    "        delta_loss = torch.abs((sae.feature_acts.grad * sae.feature_acts).view(-1, sae.feature_acts.shape[-1]).sum(dim=0))\n",
    "        topk = torch.topk(delta_loss, k=63)\n",
    "        no_ablate[sae.cfg.hook_name] = topk.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# deep copy\n",
    "import copy\n",
    "random_ablate = copy.deepcopy(no_ablate)\n",
    "\n",
    "num_features = saes[0].cfg.d_sae\n",
    "\n",
    "for key in random_ablate.keys():\n",
    "    if key == \"mask_method\":\n",
    "        continue\n",
    "#     for i in range(len(random_ablate[key])):\n",
    "#         random_ablate[key][i] = random.randint(0, num_features-1)\n",
    "# random_ablate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of predicting the correct age in the correct example\n",
      "tensor(0.0433, device='cuda:0')\n",
      "and of traceback in that example\n",
      "tensor(0.0035, device='cuda:0')\n",
      "logit difference:\n",
      "tensor(2.5023, device='cuda:0')\n",
      "probability of predicting the traceback in the error code example\n",
      "tensor(0.0041, device='cuda:0')\n",
      "and of the correct age in that example\n",
      "tensor(0.0426, device='cuda:0')\n",
      "logit difference:\n",
      "tensor(-2.3316, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test the ablation\n",
    "    \n",
    "with torch.no_grad():\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(\n",
    "                tokens,\n",
    "                cache_sae_grads=False,\n",
    "                circuit_mask=no_ablate)\n",
    "            )\n",
    "    \n",
    "    all_logit_diffs = sanity_check_model_performance(logitfn)\n",
    "    #all_logit_diffs = san(logitfn).mean()\n",
    "    # print(all_logit_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# makethe length of the dataset a multiple of the batch size\n",
    "simple_dataset = simple_dataset[:batch_size*(len(simple_dataset)//batch_size)]\n",
    "simple_labels = simple_labels[:batch_size*(len(simple_labels)//batch_size)]\n",
    "\n",
    "simple_dataset = simple_dataset.view(-1, batch_size, 65)\n",
    "simple_labels = simple_labels.view(-1, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitfn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, use_mask=True)\n",
    "        )\n",
    "\n",
    "\n",
    "def forward_pass(batch, labels, logitfn, ratio_trained=0):\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    tokens = batch\n",
    "    logits = logitfn(tokens)\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    loss = F.cross_entropy(last_token_logits, labels)\n",
    "    sparsity_loss = 0\n",
    "    for sae in saes:\n",
    "        sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "    \n",
    "    sparsity_loss = sparsity_loss / len(saes)\n",
    "\n",
    "    return loss, sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_optimized_params = []\n",
    "for sae in saes:\n",
    "    all_optimized_params.extend(list(sae.mask.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(all_optimized_params, lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  99%|█████████▉| 124/124.96000000000001 [03:23<00:01,  1.63s/it, Step=124, Progress=99.23%, Avg Nonzero Elements=246.41, Task Loss=0.2028, Sparsity Loss=4.9281] /usr/local/lib/python3.10/dist-packages/tqdm/std.py:636: TqdmWarning: clamping frac to range [0, 1]\n",
      "  full_bar = Bar(frac,\n",
      "Training Progress: 230it [06:16,  1.64s/it, Step=229, Progress=183.26%, Avg Nonzero Elements=143.31, Task Loss=0.1822, Sparsity Loss=2.8663]                                        \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m sparsity_loss\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backward pass and optimizer step\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Update tqdm bar with relevant metrics\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:483\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/overrides.py:1560\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1560\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1562\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_steps = simple_dataset.shape[0]*0.01\n",
    "\n",
    "with tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n",
    "    for i, (x, y) in enumerate(zip(simple_dataset, simple_labels)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate ratio trained\n",
    "        ratio_trained = i / total_steps\n",
    "        \n",
    "        # Update mask ratio for each SAE\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        \n",
    "        # Forward pass with updated ratio_trained\n",
    "        loss, sparsity_loss = forward_pass(x, y, logitfn, ratio_trained=0)\n",
    "        avg_nonzero_elements = sparsity_loss\n",
    "        sparsity_loss = sparsity_loss/50\n",
    "        total_loss = loss + sparsity_loss\n",
    "        \n",
    "        # Backward pass and optimizer step\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tqdm bar with relevant metrics\n",
    "        pbar.set_postfix({\n",
    "            'Step': i,\n",
    "            'Progress': f\"{ratio_trained:.2%}\",\n",
    "            \"Avg Nonzero Elements\": f\"{avg_nonzero_elements:.2f}\",\n",
    "            'Task Loss': f\"{loss.item():.4f}\",\n",
    "            'Sparsity Loss': f\"{sparsity_loss.item():.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Update the tqdm progress bar\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 202it [05:31,  1.64s/it, Step=201, Progress=107.23%, Avg Nonzero Elements=73.50, Task Loss=0.7459, Sparsity Loss=1.4700]                           \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m sparsity_loss\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backward pass and optimizer step\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Update tqdm bar with relevant metrics\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:483\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/overrides.py:1560\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1560\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1562\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_steps = simple_dataset.shape[0]*0.015\n",
    "\n",
    "with tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n",
    "    for i, (x, y) in enumerate(zip(simple_dataset, simple_labels)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate ratio trained\n",
    "        ratio_trained = i / total_steps\n",
    "        \n",
    "        # Update mask ratio for each SAE\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        \n",
    "        # Forward pass with updated ratio_trained\n",
    "        loss, sparsity_loss = forward_pass(x, y, logitfn, ratio_trained=ratio_trained)\n",
    "        avg_nonzero_elements = sparsity_loss\n",
    "        sparsity_loss = sparsity_loss/50\n",
    "        total_loss = loss + sparsity_loss\n",
    "        \n",
    "        # Backward pass and optimizer step\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tqdm bar with relevant metrics\n",
    "        pbar.set_postfix({\n",
    "            'Step': i,\n",
    "            'Progress': f\"{ratio_trained:.2%}\",\n",
    "            \"Avg Nonzero Elements\": f\"{avg_nonzero_elements:.2f}\",\n",
    "            'Task Loss': f\"{loss.item():.4f}\",\n",
    "            'Sparsity Loss': f\"{sparsity_loss.item():.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Update the tqdm progress bar\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero elements in mask for blocks.14.hook_resid_post: 74\n",
      "Nonzero elements in mask for blocks.21.hook_resid_post: 65\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.14.hook_resid_post'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saes[0].cfg.hook_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  357,   849,   850,  1401,  1641,  1672,  1788,  1919,  2186,  2567,\n",
       "          2770,  2796,  2936,  2959,  2992,  3082,  3330,  3373,  3468,  3714,\n",
       "          3742,  4141,  4149,  4160,  4300,  4472,  4698,  4713,  4718,  4736,\n",
       "          4825,  4834,  4873,  4882,  4996,  5003,  5459,  5606,  5713,  5736,\n",
       "          5741,  5932,  5938,  6150,  6205,  6400,  6436,  7216,  7435,  7570,\n",
       "          7598,  7761,  7857,  8036,  8269,  8303,  8321,  8429,  8482,  8512,\n",
       "          8746,  8930,  9368, 10044, 10244, 10512, 10545, 10550, 11150, 11333,\n",
       "         11578, 12180, 12355, 12514, 12658, 12722, 12749, 13138, 13148, 13518,\n",
       "         13585, 13691, 13770, 13958, 13962, 14057, 14187, 14201, 14309, 14502,\n",
       "         14572, 14632, 14864, 14951, 15137, 15625, 15628, 15633, 15646, 15761,\n",
       "         15849], device='cuda:0'),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(torch.sigmoid(saes[0].mask.mask*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMask()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for param in saes[0].parameters():\n",
    "    param.grad = None\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.grad = None\n",
    "\n",
    "for param in saes[0].mask.parameters():\n",
    "    param.grad = None\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero elements in mask for blocks.14.hook_resid_post: 78\n",
      "Nonzero elements in mask for blocks.21.hook_resid_post: 69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for sae in saes:\n",
    "    mask = sae.mask.mask\n",
    "    print(f\"Nonzero elements in mask for {sae.cfg.hook_name}: {torch.count_nonzero(torch.sigmoid(mask*1000))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1470337174562526"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saes[0].mask.ratio_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of predicting the correct age in the correct example\n",
      "tensor(0.6323, device='cuda:0')\n",
      "and of traceback in that example\n",
      "tensor(0.0257, device='cuda:0')\n",
      "logit difference:\n",
      "tensor(3.2191, device='cuda:0')\n",
      "probability of predicting the traceback in the error code example\n",
      "tensor(0.4098, device='cuda:0')\n",
      "and of the correct age in that example\n",
      "tensor(0.0472, device='cuda:0')\n",
      "logit difference:\n",
      "tensor(2.1808, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sanity_check_model_performance(logitfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save saes[0].mask\n",
    "torch.save(saes[0].mask, 'mask.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.30980096, 'GB')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this gives bytes, convert to GB\n",
    "torch.cuda.memory_reserved() / 1e9, \"GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
