{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c7884822e0445dabbdc5285e2cc1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "os.chdir(\"/work/pi_jensen_umass_edu/jnainani_umass_edu/CircuitAnalysisSAEs\")\n",
    "import json\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from circ4latents import data_gen\n",
    "from functools import partial\n",
    "import einops\n",
    "\n",
    "# Function to manage CUDA memory and clean up\n",
    "def cleanup_cuda():\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "# cleanup_cuda()\n",
    "# Load the config\n",
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-9b\", device=device, cache_dir=hf_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers= [7, 14, 21, 40]\n",
    "l0s = [92, 67, 129, 125]\n",
    "saes = [SAE.from_pretrained(release=\"gemma-scope-9b-pt-res\", sae_id=f\"layer_{layers[i]}/width_16k/average_l0_{l0s[i]}\", device=device)[0] for i in range(len(layers))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_diff: 6.4912004470825195\n",
      "corr_diff: -6.798116207122803\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Updated version to return JSON with more names and structure for correct and incorrect keying examples\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Expanding the name pool with a larger set of names\n",
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\"\n",
    "]\n",
    "\n",
    "for name in extended_name_pool:\n",
    "    assert len(model.tokenizer.encode(name)) == 2, f\"Name {name} has more than 1 token\"\n",
    "\n",
    "# Function to generate the dataset with correct and incorrect keying into dictionaries\n",
    "def generate_extended_dataset(name_pool, num_samples=5):\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select 5 names from the pool\n",
    "        selected_names = random.sample(name_pool, 5)\n",
    "        # Assign random ages to the selected names\n",
    "        age_dict = {name: random.randint(10, 19) for name in selected_names}\n",
    "        \n",
    "        # Create a correct example\n",
    "        correct_name = random.choice(list(age_dict.keys()))\n",
    "        correct_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> print(age[\"{correct_name}\"])\\n'\n",
    "        correct_response = age_dict[correct_name]\n",
    "        correct_token = str(correct_response)[0]\n",
    "        \n",
    "        # Create an incorrect example with a name not in the dictionary\n",
    "        incorrect_name = random.choice([name for name in name_pool if name not in age_dict])\n",
    "        incorrect_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> print(age[\"{incorrect_name}\"])\\n'\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "        \n",
    "        # Append the pair of correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# Generate the extended dataset\n",
    "json_dataset = generate_extended_dataset(extended_name_pool, num_samples=10000)\n",
    "\n",
    "# Output the JSON structure\n",
    "\n",
    "# %%\n",
    "clean_prompts = []\n",
    "corr_prompts = []\n",
    "\n",
    "answer_token = model.to_single_token(\"1\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in json_dataset[:50]:\n",
    "    corr_prompts.append(item[\"correct\"][\"prompt\"])\n",
    "    clean_prompts.append(item[\"error\"][\"prompt\"])\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompts)\n",
    "corr_tokens = model.to_tokens(corr_prompts)\n",
    "\n",
    "# %%\n",
    "def logit_diff_fn(logits):\n",
    "    err = logits[:, -1, traceback_token]\n",
    "    no_err = logits[:, -1, answer_token]\n",
    "    return (err - no_err).mean()\n",
    "\n",
    "# Disable gradients for all parameters\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "# # Compute logits for clean and corrupted samples\n",
    "logits = model(clean_tokens)\n",
    "clean_diff = logit_diff_fn(logits)\n",
    "\n",
    "logits = model(corr_tokens)\n",
    "corr_diff = logit_diff_fn(logits)\n",
    "\n",
    "print(f\"clean_diff: {clean_diff}\")\n",
    "print(f\"corr_diff: {corr_diff}\")\n",
    "\n",
    "# # Cleanup\n",
    "del logits\n",
    "cleanup_cuda()\n",
    "\n",
    "# # Define error type metric\n",
    "def _err_type_metric(logits, clean_logit_diff, corr_logit_diff):\n",
    "    patched_logit_diff = logit_diff_fn(logits)\n",
    "    return (patched_logit_diff - corr_logit_diff) / (clean_logit_diff - corr_logit_diff)\n",
    "\n",
    "err_metric_denoising = partial(_err_type_metric, clean_logit_diff=clean_diff, corr_logit_diff=corr_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_dataset = []\n",
    "# simple_labels = []\n",
    "clean_dataset = []\n",
    "corr_dataset = []\n",
    "clean_labels = []\n",
    "corr_labels = []\n",
    "\n",
    "answer_token = model.to_single_token(\"1\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in json_dataset:\n",
    "    clean_dataset.append(item[\"error\"][\"prompt\"])\n",
    "    corr_dataset.append(item[\"correct\"][\"prompt\"])\n",
    "    clean_labels.append(traceback_token)\n",
    "    corr_labels.append(answer_token)\n",
    "\n",
    "\n",
    "clean_tok_dataset = model.to_tokens(clean_dataset)\n",
    "clean_labels = torch.tensor(clean_labels)\n",
    "\n",
    "corr_tok_dataset = model.to_tokens(corr_dataset)\n",
    "corr_labels = torch.tensor(corr_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = torch.randperm(len(clean_tok_dataset))\n",
    "clean_tok_dataset = clean_tok_dataset[permutation]\n",
    "clean_labels = clean_labels[permutation]\n",
    "\n",
    "corr_tok_dataset = corr_tok_dataset[permutation]\n",
    "corr_labels = corr_labels[permutation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Mask Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import torch.nn as nn\n",
    "class KeyboardInterruptBlocker:\n",
    "    def __enter__(self):\n",
    "        # Ignore SIGINT (KeyboardInterrupt) and save the old handler\n",
    "        self.original_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore the original SIGINT handler\n",
    "        signal.signal(signal.SIGINT, self.original_handler)\n",
    "\n",
    "class SparseMask(nn.Module):\n",
    "    def __init__(self, shape, l1, seq_len=None):\n",
    "        super().__init__()\n",
    "        if seq_len is not None:\n",
    "            self.mask = nn.Parameter(torch.ones(seq_len, shape))\n",
    "        else:\n",
    "            self.mask = nn.Parameter(torch.ones(shape))\n",
    "        self.l1 = l1\n",
    "        self.max_temp = torch.tensor(1000.0)\n",
    "        self.sparsity_loss = None\n",
    "        self.ratio_trained = 1\n",
    "        self.temperature = 1\n",
    "\n",
    "\n",
    "    def forward(self, x, binary=False, mean_ablation=None):\n",
    "        if binary and mean_ablation is not None:\n",
    "            binarized = (self.mask > 0).float().to(x.device)\n",
    "            return x * binarized + mean_ablation * (~binarized.bool())\n",
    "        if binary:\n",
    "            # binary mask, 0 if negative, 1 if positive\n",
    "            binarized = (self.mask > 0).float()\n",
    "            return x * binarized\n",
    "        \n",
    "        self.temperature = self.max_temp ** self.ratio_trained\n",
    "        mask = torch.sigmoid(self.mask * self.temperature)\n",
    "        self.sparsity_loss = torch.abs(mask).sum() * self.l1\n",
    "\n",
    "        if mean_ablation is None:\n",
    "            return x * mask\n",
    "        else:\n",
    "            return x * mask + mean_ablation * (~mask.bool())\n",
    "\n",
    "for sae in saes:\n",
    "    sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=67)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_mask(mask_idxs, sae):\n",
    "    mask = torch.full_like(sae.mask.mask, -10)\n",
    "    mask[mask_idxs] = 10\n",
    "    sae.mask.mask.data = mask\n",
    "\n",
    "def load_sparsemask(mask_path):\n",
    "    json_mask = json.load(open(mask_path))\n",
    "    for sae in saes:\n",
    "        apply_mask(json_mask[sae.cfg.hook_name], sae)\n",
    "\n",
    "bos_token_id = model.tokenizer.bos_token_id\n",
    "pad_token_id = model.tokenizer.pad_token_id\n",
    "def build_sae_hook_fn(sae, sequence, cache_grads=False, circuit_mask=None, use_mask=False, binarize_mask=False, cache_masked_activations=False, cache_sae_activations=False, mean_ablate=False, mean_mask=False):\n",
    "    # make the mask for the sequence\n",
    "    mask = torch.ones_like(sequence, dtype=torch.bool)\n",
    "    mask[sequence == pad_token_id] = False\n",
    "    mask[sequence == bos_token_id] = False # where mask is false, keep original\n",
    "    def sae_hook(value, hook):\n",
    "        # print(f\"sae {sae.cfg.hook_name} running at layer {hook.layer()}\")\n",
    "        feature_acts = sae.encode(value)\n",
    "        if cache_grads:\n",
    "            sae.feature_acts = feature_acts\n",
    "            sae.feature_acts.retain_grad()\n",
    "        \n",
    "        if cache_sae_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        \n",
    "        if use_mask:\n",
    "            if mean_mask:\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask)\n",
    "\n",
    "        if circuit_mask is not None:\n",
    "            mask_method = circuit_mask['mask_method']\n",
    "            mask_indices = circuit_mask[sae.cfg.hook_name]\n",
    "            if mask_method == 'keep_only':\n",
    "                # any activations not in the mask are set to 0\n",
    "                expanded_circuit_mask = torch.zeros_like(feature_acts)\n",
    "                expanded_circuit_mask[:, :, mask_indices] = 1\n",
    "                feature_acts = feature_acts * expanded_circuit_mask\n",
    "            elif mask_method == 'zero_only':\n",
    "                feature_acts[:, :, mask_indices] = 0\n",
    "            else:\n",
    "                raise ValueError(f\"mask_method {mask_method} not recognized\")\n",
    "            \n",
    "        if cache_masked_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        if mean_ablate:\n",
    "            feature_acts = sae.mean_ablation\n",
    "\n",
    "        out = sae.decode(feature_acts)\n",
    "        # choose out or value based on the mask\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(value)\n",
    "        value = torch.where(mask_expanded, out, value)\n",
    "        return value\n",
    "    return sae_hook\n",
    "\n",
    "def build_hooks_list(sequence,\n",
    "                    cache_sae_activations=False,\n",
    "                    cache_sae_grads=False,\n",
    "                    circuit_mask=None,\n",
    "                    use_mask=False,\n",
    "                    binarize_mask=False,\n",
    "                    mean_mask=False,\n",
    "                    cache_masked_activations=False,\n",
    "                    mean_ablate=False,\n",
    "                    ):\n",
    "    hooks = []\n",
    "    # # fake hook that adds zero so gradients propagate through the model\n",
    "    param = nn.Parameter(torch.tensor(0.0, requires_grad=True))\n",
    "    hooks.append(\n",
    "        (\n",
    "            \"blocks.0.hook_resid_pre\",\n",
    "            lambda value, hook: value + param,\n",
    "        )\n",
    "    )\n",
    "    for sae in saes:\n",
    "        hooks.append(\n",
    "            (\n",
    "            sae.cfg.hook_name,\n",
    "            build_sae_hook_fn(sae, sequence, cache_grads=cache_sae_grads, circuit_mask=circuit_mask, use_mask=use_mask, binarize_mask=binarize_mask, cache_masked_activations=cache_masked_activations, cache_sae_activations=cache_sae_activations, mean_ablate=mean_ablate, mean_mask=mean_mask),\n",
    "            )\n",
    "        )\n",
    "    return hooks \n",
    "\n",
    "import gc\n",
    "def cleanup_cuda():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 67, 16384])\n",
      "torch.Size([50, 67, 16384])\n",
      "torch.Size([50, 67, 16384])\n",
      "torch.Size([50, 67, 16384])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens,\n",
    "            return_type=\"logits\",\n",
    "            fwd_hooks=build_hooks_list(tokens, cache_sae_activations=True)\n",
    "            )\n",
    "    logits = logitfn(corr_tokens)\n",
    "    for sae in saes:\n",
    "        print(sae.feature_acts.shape)\n",
    "    del logits\n",
    "    cleanup_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Level Running Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def running_mean_tensor(old_mean, new_value, n):\n",
    "    \"\"\"Update the running mean tensor using the current batch.\"\"\"\n",
    "    return old_mean + (new_value - old_mean) / n\n",
    "def get_sae_means(saes, dataset, total_steps, batch_size=16):\n",
    "    \"\"\"\n",
    "    Compute token-level means across the dataset in a batched manner.\n",
    "    Args:\n",
    "        dataset (Tensor): The input dataset of tokenized data.\n",
    "        total_steps (int): Number of steps to process.\n",
    "        batch_size (int): Number of examples per batch.\n",
    "    \"\"\"\n",
    "    for sae in saes:\n",
    "        # Initialize mean_ablation with correct shape\n",
    "        sae.mean_ablation = torch.zeros((dataset[0].shape[0], sae.cfg.d_sae)).float().to(device)\n",
    "    total_samples = len(dataset)\n",
    "    num_batches = (total_samples + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "    with tqdm(total=min(total_steps, num_batches), desc=\"Mean Accum Progress\") as pbar:\n",
    "        sample_count = 0  # To track total number of samples processed\n",
    "        for i in range(0, total_samples, batch_size):\n",
    "            # Batch selection\n",
    "            batch_x = dataset[i:i+batch_size]\n",
    "            with torch.no_grad():\n",
    "                _ = logitfn(batch_x)  # Get logits (forward pass)\n",
    "                for sae in saes:\n",
    "                    # Compute batch mean over tokens\n",
    "                    batch_mean = sae.feature_acts.mean(dim=0)  # Mean across the batch\n",
    "                    sample_count += len(batch_x)  # Update sample count\n",
    "                    # Update running mean tensor\n",
    "                    sae.mean_ablation = running_mean_tensor(\n",
    "                        sae.mean_ablation,\n",
    "                        batch_mean,\n",
    "                        sample_count\n",
    "                    )\n",
    "            pbar.update(1)  # Update progress bar\n",
    "            # Stop if we've processed enough steps\n",
    "            if i // batch_size >= total_steps:\n",
    "                break\n",
    "            cleanup_cuda()\n",
    "# get_sae_means(saes, corr_tok_dataset, 5, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Accum Progress: 6it [00:08,  1.38s/it]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logit diff: 3.799908002217611\n",
      "Circuit logit diff: -16.73712158203125\n",
      "F(C \\ K): -16.738837560017902\n",
      "F(M \\ K): 3.792098601659139\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Number of elements to remove from the last dimension\n",
    "num_remove = 5\n",
    "\n",
    "# Number of batches to process\n",
    "num_batches = 3  # Adjust this as needed\n",
    "batch_size = 16  # Batch size for processing\n",
    "\n",
    "# Remove random subsets of the mask along the [-1] dimension\n",
    "def get_indices_to_remove(mask, num_remove):\n",
    "    active_indices = (mask > 0).nonzero(as_tuple=True)[-1]  # Get indices of active elements in the last dimension\n",
    "    if len(active_indices) < num_remove:\n",
    "        raise ValueError(\"Not enough active elements to remove.\")\n",
    "    indices_to_remove = active_indices[torch.randperm(len(active_indices))[:num_remove]].to(mask.device)  # Move to the same device\n",
    "    return indices_to_remove\n",
    "\n",
    "def apply_subset_removal(mask, indices_to_remove):\n",
    "    indices_to_remove = indices_to_remove.to(mask.device)  # Ensure indices are on the same device\n",
    "    modified_mask = mask.clone()\n",
    "    modified_mask.index_fill_(-1, indices_to_remove, 0)  # Set selected elements to 0\n",
    "    return modified_mask\n",
    "\n",
    "# Circuit (random temporary mask)\n",
    "for sae in saes:\n",
    "    sae.original_mask = SparseMask(sae.cfg.d_sae, l1=1.0, seq_len=67)\n",
    "    with torch.no_grad():\n",
    "        sae.original_mask.mask.data = torch.randint(0, 2, sae.original_mask.mask.shape).float().to(device)\n",
    "\n",
    "# Generate means for the corrupted distribution\n",
    "get_sae_means(saes, corr_tok_dataset, 5, batch_size=16)\n",
    "\n",
    "# Helper function for batched processing\n",
    "def calculate_logit_diff(data, n_batches, batch_size, use_circuit):\n",
    "    total_logit_diff = 0.0\n",
    "    for batch_idx in range(n_batches):\n",
    "        # Get the batch data\n",
    "        batch_data = data[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "        with torch.no_grad():\n",
    "            logits = model.run_with_hooks(\n",
    "                batch_data,\n",
    "                return_type=\"logits\",\n",
    "                fwd_hooks=build_hooks_list(\n",
    "                    batch_data, \n",
    "                    cache_sae_activations=False, \n",
    "                    use_mask=True, \n",
    "                    binarize_mask=True, \n",
    "                    mean_mask=True\n",
    "                )\n",
    "            )\n",
    "            logit_diff = logit_diff_fn(logits).item()\n",
    "            total_logit_diff += logit_diff\n",
    "            del logits\n",
    "            cleanup_cuda()\n",
    "    # Return the average logit difference across batches\n",
    "    return total_logit_diff / n_batches\n",
    "\n",
    "# Calculate logit diff for clean and corrupted masks\n",
    "batch_clean_data = clean_tok_dataset[:batch_size * num_batches]\n",
    "cleanup_cuda()\n",
    "\n",
    "# Model logit diff (All ones mask)\n",
    "for sae in saes:\n",
    "    with torch.no_grad():\n",
    "        sae.mask.mask.data = torch.ones_like(sae.mask.mask.data).float().to(device)\n",
    "model_logit_diff = calculate_logit_diff(batch_clean_data, num_batches, batch_size, use_circuit=False)\n",
    "print(f\"Model logit diff: {model_logit_diff}\")\n",
    "\n",
    "# Circuit logit diff (Original mask)\n",
    "for sae in saes:\n",
    "    with torch.no_grad():\n",
    "        sae.mask.mask.data = sae.original_mask.mask.data\n",
    "circuit_logit_diff = calculate_logit_diff(batch_clean_data, num_batches, batch_size, use_circuit=True)\n",
    "print(f\"Circuit logit diff: {circuit_logit_diff}\")\n",
    "\n",
    "# Evaluate F(C \\ K) and F(M \\ K) for N batches\n",
    "for case in ['circuit', 'model']:\n",
    "    total_knock_logit_diff = 0.0\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_data = batch_clean_data[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "        with torch.no_grad():\n",
    "            for sae in saes:\n",
    "                # Get indices to remove based on the original mask\n",
    "                indices_to_remove = get_indices_to_remove(sae.original_mask.mask.data, num_remove)\n",
    "                if case == 'circuit':\n",
    "                    sae.mask.mask.data = apply_subset_removal(sae.original_mask.mask.data, indices_to_remove).to(device)\n",
    "                else:\n",
    "                    sae.mask.mask.data = apply_subset_removal(sae.all_ones_mask.mask.data, indices_to_remove).to(device)\n",
    "            logits = model.run_with_hooks(\n",
    "                batch_data,\n",
    "                return_type=\"logits\",\n",
    "                fwd_hooks=build_hooks_list(\n",
    "                    batch_data, \n",
    "                    cache_sae_activations=False, \n",
    "                    use_mask=True, \n",
    "                    binarize_mask=True, \n",
    "                    mean_mask=True\n",
    "                )\n",
    "            )\n",
    "            logit_diff = logit_diff_fn(logits).item()\n",
    "            total_knock_logit_diff += logit_diff\n",
    "            del logits\n",
    "            cleanup_cuda()\n",
    "    # Average logit difference for the case\n",
    "    avg_knock_logit_diff = total_knock_logit_diff / num_batches\n",
    "    if case == 'circuit':\n",
    "        print(f\"F(C \\ K): {avg_knock_logit_diff}\")\n",
    "    else:\n",
    "        print(f\"F(M \\ K): {avg_knock_logit_diff}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
