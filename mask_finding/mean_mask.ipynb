{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4440aa4deb674837a4ada826572d7961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "os.chdir(\"/work/pi_jensen_umass_edu/jnainani_umass_edu/CircuitAnalysisSAEs\")\n",
    "import json\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from circ4latents import data_gen\n",
    "from functools import partial\n",
    "import einops\n",
    "\n",
    "# Function to manage CUDA memory and clean up\n",
    "def cleanup_cuda():\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "# cleanup_cuda()\n",
    "# Load the config\n",
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-9b\", device=device, cache_dir=hf_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers= [7, 14, 21, 40]\n",
    "l0s = [92, 67, 129, 125]\n",
    "saes = [SAE.from_pretrained(release=\"gemma-scope-9b-pt-res\", sae_id=f\"layer_{layers[i]}/width_16k/average_l0_{l0s[i]}\", device=device)[0] for i in range(len(layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_diff: 3.529353141784668\n",
      "corr_diff: -4.800076007843018\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Updated version to return JSON with more names and structure for correct and incorrect keying examples\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Expanding the name pool with a larger set of names\n",
    "extended_name_pool = [\n",
    "    \"Bob\", \"Sam\", \"Lilly\", \"Rob\", \"Alice\", \"Charlie\", \"Sally\", \"Tom\", \"Jake\", \"Emily\", \n",
    "    \"Megan\", \"Chris\", \"Sophia\", \"James\", \"Oliver\", \"Isabella\", \"Mia\", \"Jackson\", \n",
    "    \"Emma\", \"Ava\", \"Lucas\", \"Benjamin\", \"Ethan\", \"Grace\", \"Olivia\", \"Liam\", \"Noah\"\n",
    "]\n",
    "\n",
    "for name in extended_name_pool:\n",
    "    assert len(model.tokenizer.encode(name)) == 2, f\"Name {name} has more than 1 token\"\n",
    "\n",
    "# Function to generate the dataset with correct and incorrect keying into dictionaries\n",
    "def generate_extended_dataset(name_pool, num_samples=5):\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly select 5 names from the pool\n",
    "        selected_names = random.sample(name_pool, 5)\n",
    "        # Assign random ages to the selected names\n",
    "        age_dict = {name: random.randint(10, 19) for name in selected_names}\n",
    "        \n",
    "        # Create a correct example\n",
    "        correct_name = random.choice(list(age_dict.keys()))\n",
    "        correct_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> var = age[\"{correct_name}\"]\\n'\n",
    "        correct_response = age_dict[correct_name]\n",
    "        correct_token = str(correct_response)[0]\n",
    "        \n",
    "        # Create an incorrect example with a name not in the dictionary\n",
    "        incorrect_name = random.choice([name for name in name_pool if name not in age_dict])\n",
    "        incorrect_prompt = f'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> age = {age_dict}\\n>>> var = age[\"{incorrect_name}\"]\\n'\n",
    "        incorrect_response = \"Traceback\"\n",
    "        incorrect_token = \"Traceback\"\n",
    "        \n",
    "        # Append the pair of correct and incorrect examples\n",
    "        dataset.append({\n",
    "            \"correct\": {\n",
    "                \"prompt\": correct_prompt,\n",
    "                \"response\": correct_response,\n",
    "                \"token\": correct_token\n",
    "            },\n",
    "            \"error\": {\n",
    "                \"prompt\": incorrect_prompt,\n",
    "                \"response\": incorrect_response,\n",
    "                \"token\": incorrect_token\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# Generate the extended dataset\n",
    "json_dataset = generate_extended_dataset(extended_name_pool, num_samples=100)\n",
    "\n",
    "# Output the JSON structure\n",
    "\n",
    "# %%\n",
    "clean_prompts = []\n",
    "corr_prompts = []\n",
    "\n",
    "answer_token = model.to_single_token(\">>>\")\n",
    "traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in json_dataset[:50]:\n",
    "    corr_prompts.append(item[\"correct\"][\"prompt\"])\n",
    "    clean_prompts.append(item[\"error\"][\"prompt\"])\n",
    "\n",
    "clean_tokens = model.to_tokens(clean_prompts)\n",
    "corr_tokens = model.to_tokens(corr_prompts)\n",
    "\n",
    "# %%\n",
    "def logit_diff_fn(logits):\n",
    "    err = logits[:, -1, traceback_token]\n",
    "    no_err = logits[:, -1, answer_token]\n",
    "    return (err - no_err).mean()\n",
    "\n",
    "# Disable gradients for all parameters\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "# # Compute logits for clean and corrupted samples\n",
    "logits = model(clean_tokens)\n",
    "clean_diff = logit_diff_fn(logits)\n",
    "\n",
    "logits = model(corr_tokens)\n",
    "corr_diff = logit_diff_fn(logits)\n",
    "\n",
    "print(f\"clean_diff: {clean_diff}\")\n",
    "print(f\"corr_diff: {corr_diff}\")\n",
    "\n",
    "# # Cleanup\n",
    "del logits\n",
    "cleanup_cuda()\n",
    "\n",
    "# # Define error type metric\n",
    "def _err_type_metric(logits, clean_logit_diff, corr_logit_diff):\n",
    "    patched_logit_diff = logit_diff_fn(logits)\n",
    "    return (patched_logit_diff - corr_logit_diff) / (clean_logit_diff - corr_logit_diff)\n",
    "\n",
    "err_metric_denoising = partial(_err_type_metric, clean_logit_diff=clean_diff, corr_logit_diff=corr_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMask(nn.Module):\n",
    "    def __init__(self, shape, l1):\n",
    "        super().__init__()\n",
    "        self.mask = nn.Parameter(torch.ones(shape))\n",
    "        self.l1 = l1\n",
    "        self.max_temp = torch.tensor(1000.0)\n",
    "        self.sparsity_loss = None\n",
    "        self.ratio_trained = 1\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        temperature = self.max_temp ** self.ratio_trained\n",
    "        mask = torch.sigmoid(self.mask * temperature)\n",
    "        self.sparsity_loss = torch.abs(mask).sum() * self.l1\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "saes[0].mask = SparseMask(saes[0].cfg.d_sae, 1.0)\n",
    "saes[1].mask = SparseMask(saes[1].cfg.d_sae, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# makethe length of the dataset a multiple of the batch size\n",
    "simple_dataset = simple_dataset[:batch_size*(len(simple_dataset)//batch_size)]\n",
    "simple_labels = simple_labels[:batch_size*(len(simple_labels)//batch_size)]\n",
    "\n",
    "simple_dataset = simple_dataset.view(-1, batch_size, 65)\n",
    "simple_labels = simple_labels.view(-1, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitfn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, use_mask=True)\n",
    "        )\n",
    "\n",
    "\n",
    "def forward_pass(batch, labels, logitfn, ratio_trained=0):\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    tokens = batch\n",
    "    logits = logitfn(tokens)\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    loss = F.cross_entropy(last_token_logits, labels)\n",
    "    sparsity_loss = 0\n",
    "    for sae in saes:\n",
    "        sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "    \n",
    "    sparsity_loss = sparsity_loss / len(saes)\n",
    "\n",
    "    return loss, sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_optimized_params = []\n",
    "for sae in saes:\n",
    "    all_optimized_params.extend(list(sae.mask.parameters()))\n",
    "\n",
    "optimizer = optim.Adam(all_optimized_params, lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_steps = simple_dataset.shape[0]*0.01\n",
    "\n",
    "with tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n",
    "    for i, (x, y) in enumerate(zip(simple_dataset, simple_labels)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate ratio trained\n",
    "        ratio_trained = i / total_steps\n",
    "        \n",
    "        # Update mask ratio for each SAE\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        \n",
    "        # Forward pass with updated ratio_trained\n",
    "        loss, sparsity_loss = forward_pass(x, y, logitfn, ratio_trained=0)\n",
    "        avg_nonzero_elements = sparsity_loss\n",
    "        sparsity_loss = sparsity_loss/50\n",
    "        total_loss = loss + sparsity_loss\n",
    "        \n",
    "        # Backward pass and optimizer step\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update tqdm bar with relevant metrics\n",
    "        pbar.set_postfix({\n",
    "            'Step': i,\n",
    "            'Progress': f\"{ratio_trained:.2%}\",\n",
    "            \"Avg Nonzero Elements\": f\"{avg_nonzero_elements:.2f}\",\n",
    "            'Task Loss': f\"{loss.item():.4f}\",\n",
    "            'Sparsity Loss': f\"{sparsity_loss.item():.4f}\"\n",
    "        })\n",
    "        \n",
    "        # Update the tqdm progress bar\n",
    "        pbar.update(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
